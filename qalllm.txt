ูุญุชููุงุช ูููุงุช ููุงุฐุฌ PyTorch ุงููุญููุธุฉ
ูู ุงูุชุฏุฑูุจ ุงูุชุฃุณูุณู ููููุฐุฌ ูุบูู. ูุงุฐุง ูุง ุชุฑู ูุฑู ูู ููู ููุทุฉ ุชุธุฑูุจูุฉ ูู ููุน .pt ุงู ูู ูุฑุฃูุง ุงูููู ููุงุฐุง ุณูุฌุฏ
ูู ููู .pt (ููู ุงูุชุฏุงุฏ ุดุงุฆุน ูููุงุฐุฌ PyTorch ุงููุญููุธุฉ)ุ ุณุชุฌุฏ ุนุงุฏุฉู ุฃุญุฏ ุงูููุงูู ุงูุชุงููุฉ ุนูุฏ ูุฑุงุกุชู:

1. ุงูุฃูุซุฑ ุดููุนุงู: ูุงููุณ (Dictionary)
ูุญุชูู ุนูู ููุงุชูุญ ุฑุฆูุณูุฉ ูุซู:

python
{
    'epoch': 150,                    # ุฑูู ุงูุฏูุฑุฉ ุงูุชุฏุฑูุจูุฉ
    'model_state_dict': {...},       # ุฃูุฒุงู ุงููููุฐุฌ
    'optimizer_state_dict': {...},   # ุญุงูุฉ ุงูููุญุณูู (ูุนุฏูุงุช ุงูุชุนููุ ุงูุฒุฎู...)
    'loss': 0.05,                    # ูููุฉ ุงูุฎุณุงุฑุฉ ุงูุฃุฎูุฑุฉ
    'config': {...},                 # ุฅุนุฏุงุฏุงุช ุงููููุฐุฌ (ุงููุนูุงุฑูุฉุ ุงููุนููุงุช)
}
2. ูุญุชููุงุช ููุตูุฉ:
ุฃ. ุญุงูุฉ ุงููููุฐุฌ (Model State Dict)
ุฃูุฒุงู ุงูุทุจูุงุช: ูุตูููุงุช ุงูุฃูุฒุงู ูุงูุงูุญูุงุฒุงุช ููู ุทุจูุฉ

ุฅุญุตุงุกุงุช BatchNorm: ุงููุชูุณุท ูุงูุชุจุงูู ุงููุชุญุฑู

ุญุงูุฉ RNN/LSTM: ุฃูุฒุงู ุงูุฎูุงูุง ุงููุชูุฑุฑุฉ

ุชุถูููุงุช ุงููููุงุช (Embeddings): ูุตูููุฉ ุงูุชุถููู

ุจ. ุญุงูุฉ ุงูููุญุณูู (Optimizer State)
ูุนุฏูุงุช ุงูุชุนูู ุงูุชููููุฉ (ูู Adamุ Adagrad...)

ุนุฒู ุงูุญุฑูุฉ (Momentum) buffers

ุฅุญุตุงุกุงุช ุงูุชุฏุฑุฌ

ุฌ. ูุนูููุงุช ุงูุชุฏุฑูุจ
ุณุฌู ุงูุฎุณุงุฑุฉ (Loss history)

ุฏูุฉ ุงูุชุฏุฑูุจ/ุงูุงุฎุชุจุงุฑ

ูุคุดุฑุงุช ุงูุฃุฏุงุก ุงูุฃุฎุฑู

ุฏ. ููุชุงุฏุงุชุง (Metadata)
ุฅุตุฏุงุฑ PyTorch

ุชุงุฑูุฎ ุงูุญูุธ

ูุนูููุงุช ุงููุนูุงุฑูุฉ

ูุฆุงุช ุงูููุฑุฏุงุช (Vocabulary)

3. ููู ุชูุฑุฃ ุงูููู:
python
import torch

# ุชุญููู ุงูููู
checkpoint = torch.load('model.pt', map_location='cpu')

# ุนุฑุถ ุงูููุงุชูุญ
print(checkpoint.keys())

# ุงููุตูู ููุฃูุฒุงู
model_weights = checkpoint['model_state_dict']
for name, param in model_weights.items():
    print(f"{name}: {param.shape}")
4. ููุงุญุธุงุช ูููุฉ:
ุงูููู ุซูุงุฆู (binary) ูุง ูููู ูุฑุงุกุชู ููุต ุนุงุฏู

ุงูุญุฌู ูุฏ ูุชุฑุงูุญ ูู ููุบุงุจุงูุช ุฅูู ุนุดุฑุงุช ุงูุฌูุฌุงุจุงูุช ุญุณุจ ุญุฌู ุงููููุฐุฌ

ูุฏ ูุญุชูู ุนูู ููุณูุณูุงุช (Serialized objects) ูุงููุฉ

ุฃุญูุงูุงู ูููู ุงูููู ูุฌุฑุฏ ุฃูุฒุงู ููุท ุจุฏูู ููุชุงุฏุงุชุง

5. ุฃูุซูุฉ ุนูู ูุง ูุฏ ุชุฌุฏู ูู ุงูููุงุฐุฌ ุงููุบููุฉ:
ุฃูุฒุงู ุทุจูุงุช ุงูุงูุชุจุงู (Attention)

ูุตูููุงุช ุงูุชุญููู ุงูุฎุทู (Linear transformations)

ุฃูุฒุงู ุงูุชุทุจูุน ุงูุทุจูู (LayerNorm)

ุชุถูููุงุช ุงูููุถุนูุฉ (Positional embeddings)

ูู ุชุฑูุฏ ูุนุฑูุฉ ููููุฉ ุงุณุชุฎุฑุงุฌ ูุนูููุงุช ูุญุฏุฏุฉ ูู ูุฐุง ุงููููุ

ูุนู
ุฃุนููู ุจุงูุชูุตูู ููู ุชุณุชุฎุฑุฌ ูุชุญูู ูุญุชููุงุช ููู .pt:

1. ุชุญููู ุงูููู ูุชุญููู ุงููููู ุงูุฃุณุงุณู
python
import torch
import json
import numpy as np
from collections import OrderedDict

# ุชุญููู ุงูููู ูุน ุชุญุฏูุฏ ุงูุฌูุงุฒ (ุงูุชุฑุถ CPU ููุณูุงูุฉ)
checkpoint = torch.load('model.pt', map_location=torch.device('cpu'))

# 1. ูุนุฑูุฉ ููุน ุงูุจูุงูุงุช ุงูุฃุณุงุณู
print(f"ููุน ุงูุจูุงูุงุช: {type(checkpoint)}")

# 2. ุฅุฐุง ูุงู ูุงููุณุงู - ุนุฑุถ ุงูููุงุชูุญ
if isinstance(checkpoint, dict):
    print("\n๐ ุงูููุงุชูุญ ุงูุฑุฆูุณูุฉ ูู ุงูููู:")
    for key in checkpoint.keys():
        print(f"  - {key}")
    
    # 3. ุชุญููู ูู ููุชุงุญ
    for key, value in checkpoint.items():
        print(f"\n๐ ุชุญููู ุงูููุชุงุญ: '{key}'")
        print(f"   ุงูููุน: {type(value)}")
        
        if isinstance(value, (int, float, str, bool)):
            print(f"   ุงููููุฉ: {value}")
        
        elif torch.is_tensor(value):
            print(f"   ุชูุณูุฑ | ุงูุดูู: {value.shape} | ุงูููุน: {value.dtype}")
            print(f"   ุงูุฅุญุตุงุฆูุงุช: min={value.min():.4f}, max={value.max():.4f}")
            print(f"   ุงููุชูุณุท: {value.mean():.4f}")
        
        elif isinstance(value, dict):
            print(f"   ูุงููุณ | ุนุฏุฏ ุงูุนูุงุตุฑ: {len(value)}")
            # ุนุฑุถ ุฃูู 3 ููุงุชูุญ ุฅุฐุง ูุงู ูุจูุฑุงู
            items = list(value.items())
            for sub_key, sub_val in items[:3]:
                if torch.is_tensor(sub_val):
                    print(f"     '{sub_key}': ุชูุณูุฑ {sub_val.shape}")
                else:
                    print(f"     '{sub_key}': {type(sub_val)}")
2. ุชุญููู ุนููู ูุฃูุฒุงู ุงููููุฐุฌ (Model State Dict)
python
def analyze_model_weights(state_dict):
    """ุชุญููู ููุตู ูุฃูุฒุงู ุงููููุฐุฌ"""
    
    print("๐ ุชุญููู ุฃูุฒุงู ุงููููุฐุฌ:")
    print("=" * 60)
    
    # ุชุตููู ุงูุทุจูุงุช ุญุณุจ ุงูููุน
    layer_types = {}
    total_params = 0
    total_size_bytes = 0
    
    for name, param in state_dict.items():
        # ุนุฏ ุงููุนููุงุช
        num_params = param.numel() if torch.is_tensor(param) else 0
        param_size = param.element_size() * num_params if torch.is_tensor(param) else 0
        
        total_params += num_params
        total_size_bytes += param_size
        
        # ุชุตููู ุญุณุจ ุงุณู ุงูุทุจูุฉ
        if 'weight' in name:
            layer_name = name.replace('.weight', '')
            layer_type = layer_name.split('.')[-1] if '.' in layer_name else layer_name
            layer_types[layer_type] = layer_types.get(layer_type, 0) + num_params
        
        # ุนุฑุถ ูุนูููุงุช ูููุฉ
        if any(x in name for x in ['embed', 'word_emb', 'token_emb']):
            print(f"\n๐ค ุชุถููู ุงููููุงุช: {name}")
            print(f"   ุงูุดูู: {param.shape}")  # (vocab_size, hidden_size)
            print(f"   ุงูููุฑุฏุงุช: {param.shape[0]} ูููุฉ")
            print(f"   ุงูุจุนุฏ ุงููุฎูู: {param.shape[1]} ุจูุนุฏ")
        
        elif 'attention' in name.lower():
            print(f"\n๐ฏ ุงูุชุจุงู: {name}")
            print(f"   ุงูุดูู: {param.shape}")
            
        elif 'norm' in name.lower():
            print(f"\n๐ ุชุทุจูุน: {name}")
            print(f"   ุงูุดูู: {param.shape}")
    
    print(f"\n{'='*60}")
    print(f"๐ ุงูุฅุญุตุงุฆูุงุช ุงูุฅุฌูุงููุฉ:")
    print(f"   ุฅุฌูุงูู ุงููุนููุงุช: {total_params:,}")
    print(f"   ุฅุฌูุงูู ุงูุญุฌู: {total_size_bytes / 1e6:.2f} MB")
    print(f"   ุนุฏุฏ ุงูุทุจูุงุช: {len(state_dict)}")
    
    return total_params
3. ุงุณุชุฎุฑุงุฌ ุชูููู ุงููููุฐุฌ (Configuration)
python
def extract_model_config(checkpoint):
    """ุงุณุชุฎุฑุงุฌ ุฅุนุฏุงุฏุงุช ุงููููุฐุฌ"""
    
    config = {}
    
    # ุงูุจุญุซ ุนู ุงูุชูููู ูู ุฃูุงูู ูุญุชููุฉ
    config_keys = ['config', 'args', 'params', 'hyperparameters', 'model_config']
    
    for key in config_keys:
        if key in checkpoint and isinstance(checkpoint[key], dict):
            config = checkpoint[key]
            print(f"\nโ๏ธ  ุงูุชูููู ูุฌุฏ ูู ุงูููุชุงุญ: '{key}'")
            break
    
    if config:
        print("\n๐ง ุฅุนุฏุงุฏุงุช ุงููููุฐุฌ:")
        for k, v in config.items():
            if isinstance(v, (int, float, str, bool, list, dict)):
                print(f"  {k}: {v}")
            else:
                print(f"  {k}: {type(v)}")
    else:
        print("\nโ๏ธ  ูู ูุชู ุงูุนุซูุฑ ุนูู ุชูููู ุตุฑูุญ. ูุญุงููุฉ ุงูุงุณุชุฏูุงู...")
        infer_config_from_weights(checkpoint.get('model_state_dict', {}))
    
    return config

def infer_config_from_weights(state_dict):
    """ุงุณุชูุชุงุฌ ุฅุนุฏุงุฏุงุช ุงููููุฐุฌ ูู ุงูุฃูุฒุงู"""
    print("\n๐ค ุงุณุชูุชุงุฌ ุงูุฅุนุฏุงุฏุงุช ูู ุงูุฃูุฒุงู:")
    
    # ุงูุจุญุซ ุนู ุชุถูููุงุช ุงููููุงุช
    for name, param in state_dict.items():
        if 'embed' in name and len(param.shape) == 2:
            vocab_size, hidden_size = param.shape
            print(f"  - ุญุฌู ุงูููุฑุฏุงุช: {vocab_size}")
            print(f"  - ุงูุจุนุฏ ุงููุฎูู: {hidden_size}")
            break
    
    # ุนุฏ ุทุจูุงุช ุงูุงูุชุจุงู
    attention_layers = [k for k in state_dict.keys() if 'attention' in k and 'weight' in k]
    num_layers = len(set([k.split('.')[1] for k in attention_layers if '.' in k])) if attention_layers else 0
    if num_layers:
        print(f"  - ุนุฏุฏ ุงูุทุจูุงุช: {num_layers}")
4. ุชุญููู ุญุงูุฉ ุงููุญุณู (Optimizer State)
python
def analyze_optimizer_state(optimizer_dict):
    """ุชุญููู ุญุงูุฉ ูุญุณู ุงูุชุฏุฑูุจ"""
    
    if not optimizer_dict:
        print("\nโ๏ธ  ูุง ุชูุฌุฏ ุญุงูุฉ ูููุญุณู")
        return
    
    print("\nโก ุชุญููู ุญุงูุฉ ุงููุญุณู:")
    print(f"   ููุน ุงููุญุณู: {optimizer_dict.get('__class__', 'ุบูุฑ ูุนุฑูู')}")
    
    # ุชุญููู ุงูุญุงูุฉ
    if 'state' in optimizer_dict:
        print(f"   ุนุฏุฏ ุงููุนููุงุช ุงููุชุงุจุนุฉ: {len(optimizer_dict['state'])}")
        
        # ุนุฑุถ ุญุงูุฉ ุฃูู ูุนููุฉ
        first_key = next(iter(optimizer_dict['state']))
        first_state = optimizer_dict['state'][first_key]
        
        print(f"\n   ูุซุงู ุนูู ุญุงูุฉ ูุนููุฉ:")
        for k, v in first_state.items():
            if torch.is_tensor(v):
                print(f"     {k}: ุชูุณูุฑ {v.shape} | ูุชูุณุท: {v.mean():.6f}")
5. ุฏูุฑุฉ ุชุฏุฑูุจ ูุงููุฉ
python
def full_checkpoint_analysis(checkpoint_path):
    """ุชุญููู ูุงูู ูููู ููุทุฉ ุงูุชุธุฑูุจูุฉ"""
    
    print(f"๐ ุจุฏุก ุชุญููู ุงูููู: {checkpoint_path}")
    print("=" * 70)
    
    # 1. ุชุญููู ุงูููู
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # 2. ุชุญููู ุนุงู
    print(f"ููุน ุงููุงุฆู: {type(checkpoint)}")
    
    if isinstance(checkpoint, dict):
        # 3. ุงุณุชุฎุฑุงุฌ ุงูููููุงุช
        model_dict = checkpoint.get('model_state_dict', 
                                  checkpoint.get('model', 
                                               checkpoint.get('state_dict', {})))
        
        # 4. ุชุญููู ุงูุฃูุฒุงู
        if model_dict:
            total_params = analyze_model_weights(model_dict)
        
        # 5. ุชุญููู ุงูุชูููู
        config = extract_model_config(checkpoint)
        
        # 6. ูุนูููุงุช ุงูุชุฏุฑูุจ
        print("\n๐ ูุนูููุงุช ุงูุชุฏุฑูุจ:")
        for key in ['epoch', 'step', 'iteration', 'global_step']:
            if key in checkpoint:
                print(f"  {key}: {checkpoint[key]}")
        
        if 'loss' in checkpoint:
            print(f"  ุงูุฎุณุงุฑุฉ: {checkpoint['loss']:.4f}")
        
        # 7. ุงููุญุณู
        optimizer_dict = checkpoint.get('optimizer_state_dict', {})
        analyze_optimizer_state(optimizer_dict)
        
        # 8. ุญูุธ ุงููุนูููุงุช ููุงุณุชุฎุฏุงู ูุงุญูุงู
        save_analysis_report(checkpoint, checkpoint_path)
    
    elif torch.is_tensor(checkpoint):
        print("๐ฆ ุงูููู ูู ุชูุณูุฑ ูุจุงุดุฑ:")
        print(f"   ุงูุดูู: {checkpoint.shape}")
        print(f"   ุงูููุน: {checkpoint.dtype}")
    
    print("\nโ ุงูุชูู ุงูุชุญููู")

def save_analysis_report(checkpoint, filename):
    """ุญูุธ ุชูุฑูุฑ ุงูุชุญููู"""
    report = {
        'total_size_mb': sum(p.numel() * p.element_size() 
                           for p in checkpoint.values() 
                           if torch.is_tensor(p)) / 1e6,
        'keys': list(checkpoint.keys()),
        'analysis_date': str(datetime.now())
    }
    
    report_name = f"analysis_{filename.split('/')[-1].replace('.pt', '.json')}"
    with open(report_name, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\n๐พ ุชู ุญูุธ ุงูุชูุฑูุฑ ูู: {report_name}")
6. ููููุฉ ุงูุงุณุชุฎุฏุงู
python
# ุงูุงุณุชุฎุฏุงู ุงูุฃุณุงุณู
full_checkpoint_analysis('bert-model.pt')

# ุฃู ุฎุทูุฉ ุจุฎุทูุฉ
checkpoint = torch.load('model.pt', map_location='cpu')

# 1. ุฑุคูุฉ ูุญุชููุงุช ูุญุฏุฏุฉ
print("ุฃูุฒุงู ุชุถููู ุงููููุงุช:")
embeddings = checkpoint['model_state_dict']['embeddings.word_embeddings.weight']
print(f"  ุงูุดูู: {embeddings.shape}")
print(f"  ุงููููุฉ 100: {embeddings[100][:10]}...")  # ุฃูู 10 ุฃุจุนุงุฏ

# 2. ุชุญููู ุงูุชูุณูุฑ ุฅูู numpy ููุชุตูุฑ
import matplotlib.pyplot as plt

# ุชุตูุฑ ุชูุฒูุน ุงูุฃูุฒุงู
weights = checkpoint['model_state_dict']['encoder.layer.0.attention.self.query.weight']
plt.hist(weights.flatten().numpy(), bins=50)
plt.title("ุชูุฒูุน ุฃูุฒุงู ุทุจูุฉ ุงูุงูุชุจุงู")
plt.show()
7. ูุตุงุฆุญ ูููุฉ
ุงุณุชุฎุฏู map_location='cpu' ุฏุงุฆูุงู ูุชุฌูุจ ูุดุงูู ุงูุฐุงูุฑุฉ

ุงูุญุต ุงูุญุฌู ุฃููุงู ุจุงุณุชุฎุฏุงู os.path.getsize('model.pt')

ูู ุงูููุงุฐุฌ ุงููุจูุฑุฉุ ูุง ุชุญููู ูู ุดูุก ูุฑุฉ ูุงุญุฏุฉ

ุงุญูุธ ูุณุฎุฉ ูุจู ุงูุชุนุฏูู

ุงุณุชุฎุฏู torch.save(checkpoint, 'new_model.pt', pickle_protocol=4) ููุญูุธ ุงูุฃูุซู

8. ุฃูุซูุฉ ุนูู ุงุณุชุฎุฑุงุฌุงุช ูููุฏุฉ
python
# ุงุณุชุฎุฑุงุฌ ุงูููุฑุฏุงุช ูู ูููุฐุฌ ูุบูู
if 'vocab' in checkpoint:
    vocab = checkpoint['vocab']
    print(f"ุฃูู 10 ูููุงุช: {list(vocab.items())[:10]}")

# ุงุณุชุฎุฑุงุฌ ุฅุญุตุงุกุงุช ุงูุชุฏุฑูุจ
if 'history' in checkpoint:
    losses = checkpoint['history']['loss']
    print(f"ูุนุฏู ุงูุฎุณุงุฑุฉ: {np.mean(losses[-10:]):.4f}")

# ุชุญูู ูู ูุงุจููุฉ ุงูุงุณุชุฎุฏุงู
if all(k in checkpoint for k in ['model_state_dict', 'config', 'optimizer_state_dict']):
    print("โ ุงูููู ูุงูู ููุตูุญ ูุงุณุชุฆูุงู ุงูุชุฏุฑูุจ")
else:
    print("โ๏ธ  ุงูููู ูุงูุต ุจุนุถ ุงูููููุงุช")
ูู ุชุฑูุฏ ุชุญููู ููู ูุญุฏุฏ ุฃู ูุฏูู ุณุคุงู ุนู ุดูุก ูุนูู ุชุฌุฏู ูู ูููุงุช .ptุ

ูููู ูุฏู ุณุคุงู ููู.. ุงุฑูุฏ ุงู ุงููู ููู ุชุชููู ุงูุงูุฒุงู ูู ุงูููู. ุงูุง ุงุนูู ุงู ููู ูููุฉ ูุชุฌููุง ุงูุฐู ููุซู ูุฌููุนุฉ ุงูุฒุงู ูู ูุฒู ูู ููุฉ ูุญุชูุงุฆูุง ูู ูุนูู ูููุฉ ุงุฎุฑู ูุฏุฑุฌุฉ ุงุฑุชุจุงุทูุง ุจู. ุณุคุงู ูู.. ูู ูุนูู ุฐูู ุงููุง ูุซูุง ุญูู ููุชุญ ูููุฉ .. ูุงู .. ููู ุณูุฌุฏ ุงูู ุฎุงูุฉ ุนูู ุงููููู ุงู ุงููุณุงุฑ ุชุญูู ุงููููุฉ ููุณูุง ููู ุซู ุฎุงูุฉ ูุฒู ุชูุซู ูุซูุง ููุฉ ุนูุงูุชูุง ุจูููุฉ .  ุตุฑู.. ูุฎููุฉ ูุฌุงูุฑุฉ ูููุฉ ุนูุงูุชูุง ุจูููุฉ .. ุฑุจุญ.. ูููุฐุงุ
ุณุคุงู ููุชุงุฒ ูุฌููุฑู! ุฏุนูู ุฃูุถุญ ูุฐุง ุงูููููู ุจุฏูุฉ ุดุฏูุฏุฉุ ูุฃู ูููู ุจู ุจุนุถ ุงููุบุงูุทุฉ ุงููููุฉ.

ุงูุญูููุฉ: ุงูุฃูุฑ ูุฎุชูู ุชูุงูุงู ุนูุง ุชุชุฎูู!
โ ูุง ูู ุบูุฑ ุตุญูุญ:
ูุง ููุฌุฏ ูู ููู ุงูุฃูุฒุงู ุฃู ูููุงุช ูุตูุฉ ููุง ุนูุงูุงุช ูุงุถุญุฉ ุจูู ูููุฉ ูุฃุฎุฑู ูุฎุฒูุฉ ุจุดูู ูุจุงุดุฑ.

โ ูุง ูู ุงูุตุญูุญ:
1. ููู ุชุฎุฒู ุงูููุงุฐุฌ ุงููุบููุฉ ุงููุนุงููุ
text
ูููุฉ "ูุงู" โ [0.12, -0.45, 0.87, -0.23, 0.56, ...]  # ูุชุฌู ุจุทูู 768 ุจูุนุฏ (ูุซูุงู)
ูููุฉ "ุตุฑู" โ [0.34, 0.12, -0.67, 0.89, -0.45, ...]  # ูุชุฌู ูุฎุชูู ุชูุงูุงู
ูู ุจูุนุฏ ูู ูุฐุง ุงููุชุฌู ููุซู ุณูุฉ ูุฌุฑุฏุฉุ ููุณ ูููุฉ ูุญุฏุฏุฉ!

2. ูุง ุงูุฐู ูุฑุงู ูุนูุงู ูู ููู .ptุ
ูููุชุฑุถ ูููุฐุฌุงู ุจุจุนุฏ ูุฎูู = 4 (ุจุณูุท ููุชูุถูุญ):

python
# ูุฐุง ูุง ูุฌุฏู ูุนูุงู ูู ููู .pt:
embeddings_tensor = torch.tensor([
    [0.12, -0.45, 0.87, -0.23],  # ูููุฉ 0 (ูุซูุงู "[PAD]")
    [0.34, 0.12, -0.67, 0.89],   # ูููุฉ 1 (ูุซูุงู "[UNK]")
    [0.56, -0.78, 0.34, -0.12],  # ูููุฉ 2 (ูุซูุงู "ูุงู")
    [0.78, 0.45, -0.23, 0.67],   # ูููุฉ 3 (ูุซูุงู "ุตุฑู")
    [0.23, -0.67, 0.89, -0.34],  # ูููุฉ 4 (ูุซูุงู "ุฑุจุญ")
    # ... ุขูุงู ุงูุตููู ุงูุฃุฎุฑู
])
ููุงุญุธุฉ ูููุฉ: ุงูููุฑุณ (ุงูุตู) ููุท ูู ุงูุฐู ูุดูุฑ ูููููุฉุ ูููุณ ุงููุญุชูู!

3. ููู ุชุนุฑู ุงูููุฐุฌุฉ ุงูุนูุงูุงุช ุจูู ุงููููุงุชุ
ุฃ. ุงูููุฑุฏุงุช ุงููููุตูุฉ (Vocabulary):
ููุฌุฏ ููู ูููุตู ุฃู ูุณู ูู ุงูููู ูุฑุจุท ุงูููุฑุณ ุจุงููููุฉ:

python
vocab = {
    0: "[PAD]",
    1: "[UNK]", 
    2: "ูุงู",
    3: "ุตุฑู", 
    4: "ุฑุจุญ",
    # ...
}

# ูููุฉ "ูุงู" โ ุงูููุฑุณ 2 โ ุงูุตู 2 ูู ูุตูููุฉ ุงูุชุถููู
ุจ. ุงูุนูุงูุงุช ุชูุณุชูุชุฌ ูุง ุชูุฎุฒู:
ุงูุนูุงูุฉ ุจูู "ูุงู" ู"ุตุฑู" ูุง ุชูุฎุฒู ุตุฑุงุญุฉุ ุจู ุชูุญุณุจ ุนูุฏ ุงูุชุดุบูู:

python
# ุญุณุงุจ ุงูุชุดุงุจู (ุงูุนูุงูุฉ) ุจูู ูููุชูู
def similarity(word1_idx, word2_idx):
    vec1 = embeddings_tensor[word1_idx]  # ูุชุฌู "ูุงู"
    vec2 = embeddings_tensor[word2_idx]  # ูุชุฌู "ุตุฑู"
    
    # ุญุงุตู ุงูุถุฑุจ ุงูููุทู (ููุงุณ ุงูุชุดุงุจู)
    similarity = torch.dot(vec1, vec2)
    
    # ุฃู ุฌูุจ ุงูุชูุงู
    cos_sim = torch.cosine_similarity(vec1, vec2, dim=0)
    
    return cos_sim.item()

# ุงูุชุดุงุจู ุจูู "ูุงู" ู"ุตุฑู"
sim = similarity(vocab["ูุงู"], vocab["ุตุฑู"])  # โ 0.85 (ุนุงูู)

# ุงูุชุดุงุจู ุจูู "ูุงู" ู"ุณูุงุฑุฉ"
sim2 = similarity(vocab["ูุงู"], vocab["ุณูุงุฑุฉ"])  # โ 0.12 (ููุฎูุถ)
4. ูุซุงู ุนููู: ูููุชุด ูู ููู .pt ุญูููู
python
import torch

# ุชุญููู ุงููููุฐุฌ
checkpoint = torch.load('model.pt', map_location='cpu')

# 1. ุฅูุฌุงุฏ ูุตูููุฉ ุงูุชุถููู
embeddings = checkpoint['model.embeddings.word_embeddings.weight']
print(f"ุดูู ูุตูููุฉ ุงูุชุถููู: {embeddings.shape}")
# โ torch.Size([50000, 768]) ูุนูู 50,000 ูููุฉ ร 768 ุจูุนุฏุงู

# 2. ุงูููุฑุฏุงุช (ุนุงุฏุฉ ูู ููุงู ูููุตู)
vocab = checkpoint.get('vocab', {})
# ุฃู ูุฏ ุชููู ูู ููู ูุตู ูููุตู

# 3. ุงูุจุญุซ ุนู ูููุฉ "ูุงู"
if 'ูุงู' in vocab:
    word_idx = vocab['ูุงู']  # ูููุฑุถ = 247
    word_vector = embeddings[247]  # ุงููุชุฌู ุงููุงูู ูููููุฉ
    
    print(f"\nูุชุฌู ูููุฉ 'ูุงู' (ุฃูู 10 ุฃุจุนุงุฏ ูู 768):")
    print(word_vector[:10])
    # โ tensor([0.124, -0.452, 0.876, -0.234, 0.561, ...])
    
    # 4. ุงูุจุญุซ ุนู ุงููููุงุช ุงูุฃูุฑุจ ููุง
    print(f"\nุงููููุงุช ุงูุฃูุฑุจ ุฅูู 'ูุงู':")
    find_nearest_words(embeddings, word_idx, vocab)

def find_nearest_words(embeddings, target_idx, vocab, top_k=5):
    """ุฅูุฌุงุฏ ุงููููุงุช ุงูุฃูุซุฑ ุชุดุงุจูุงู"""
    target_vec = embeddings[target_idx]
    
    similarities = []
    for idx, vec in enumerate(embeddings):
        if idx != target_idx:
            sim = torch.cosine_similarity(target_vec, vec, dim=0)
            similarities.append((idx, sim.item()))
    
    # ุชุฑุชูุจ ุชูุงุฒูู ุญุณุจ ุงูุชุดุงุจู
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    print("ุงููููุงุช ุงูุฃูุซุฑ ุชุดุงุจูุงู:")
    for i in range(top_k):
        word_idx, sim_score = similarities[i]
        # ุงูุจุญุซ ุนู ุงููููุฉ ุงูููุงุจูุฉ ูููุนุฑู (ูุฐู ุชุญุชุงุฌ ุฎุฑูุทุฉ ุนูุณูุฉ)
        word = [k for k, v in vocab.items() if v == word_idx][0]
        print(f"  {word}: {sim_score:.3f}")
5. ูุง ุงูุฐู ุชูุซูู ุงูุฃุจุนุงุฏ ูุนูุงูุ
text
ุงูุจุนุฏ 0: ูุฏ ููุซู "ุงูุญุฏุงุซุฉ vs ุงููุฏู"
ุงูุจุนุฏ 1: ูุฏ ููุซู "ุงูุฅูุฌุงุจูุฉ vs ุงูุณูุจูุฉ"  
ุงูุจุนุฏ 2: ูุฏ ููุซู "ุงูุชุฌุฑูุฏ vs ุงููุงุฏูุฉ"
ุงูุจุนุฏ 3: ูุฏ ููุซู "ุงูููุฉ vs ุงูุถุนู"
... ูููุฐุง
ููู ูุง ูููู ุชูุณูุฑูุง ุจุณูููุฉุ ูุฃููุง ุชุชุนูู ุชูุฒูุนุงุช ุฅุญุตุงุฆูุฉ ูุนูุฏุฉ.

6. ุชุณูุณู ุงููุนุงูุฌุฉ ุงููุงูู:
text
1. ุงููููุฉ ุงููุตูุฉ "ูุงู" 
   โ
2. ุงูุจุญุซ ูู ุงูููุฑุฏุงุช โ ุงูููุฑุณ 247
   โ
3. ุงุณุชุฑุฌุงุน ุงูุตู 247 ูู ูุตูููุฉ ุงูุชุถููู
   โ
4. ุงูุญุตูู ุนูู ุงููุชุฌู: [0.12, -0.45, 0.87, -0.23, ...]
   โ
5. ุชูุฑูุฑ ุงููุชุฌู ุนุจุฑ ุทุจูุงุช ุงูุงูุชุจุงู
   โ
6. ุญุณุงุจ ุงูุชูุงุนูุงุช ูุน ูููุงุช ุงูุณูุงู
7. ููุชุฎูู ุงูุฃูุฑ ุจุดูู ููููุณ:
ุชุฎูู ุฃู ูุฏููุง ููุชุจุฉ ุถุฎูุฉ:

ุงูุฑููู (ุงูุตููู): ูู ุฑู ููุซู ูููุฉ

ุงููุชุจ ุนูู ุงูุฑู (ุงูุฃุนูุฏุฉ): ูู ูุชุงุจ ููุซู ุณูุฉ ูุฌุฑุฏุฉ

ูุง ููุฌุฏ ูุชุงุจ ุจุนููุงู "ุนูุงูุฉ ูุน ุตุฑู"ุ ุจู ูุชุจ ูุฌุฑุฏุฉ ูุซู "ุงูุงูุชุตุงุฏ"ุ "ุงููููุฉ"ุ "ุงููููุฉ"...

ุนูุฏ ุงูุจุญุซ ุนู ุนูุงูุฉ "ูุงู" ูุน "ุตุฑู":

ูุฐูุจ ูุฑู "ูุงู" ููุฃุฎุฐ ุฌููุน ูุชุจู

ูุฐูุจ ูุฑู "ุตุฑู" ููุฃุฎุฐ ุฌููุน ูุชุจู

ููุงุฑู ูุฌููุนุงุช ุงููุชุจ ููุฑู ูุฏู ุชุดุงุจููุง

8. ุชุทุจูู ุนููู ุตุบูุฑ:
python
# ููุฎูู ูููุฐุฌุงู ุตุบูุฑุงู ูููู ุงูููุฑุฉ
vocab_size = 10
embedding_dim = 3

# ูุตูููุฉ ุชุถููู ุนุดูุงุฆูุฉ (ูู ุงูุชุฏุฑูุจ ุงููุนูู ุชุชุนูู ูู ุงูุจูุงูุงุช)
embeddings = torch.randn(vocab_size, embedding_dim)

# ุงูููุฑุฏุงุช ุงููุตุบุฑุฉ
vocab = {
    "ูุงู": 0,
    "ุตุฑู": 1, 
    "ุฑุจุญ": 2,
    "ุฎุณุงุฑุฉ": 3,
    "ุณูุงุฑุฉ": 4,
    "ุจูุช": 5,
    "ุทุนุงู": 6,
    "ุดุฑุงุก": 7,
    "ุจูุน": 8,
    "ุณุนุฑ": 9
}

# ุฎุฑูุทุฉ ุนูุณูุฉ
idx_to_word = {v: k for k, v in vocab.items()}

# ุญุณุงุจ ุงูุชุดุงุจู ุจูู ูู ุงููููุงุช
print("ูุตูููุฉ ุงูุชุดุงุจู ุจูู ุงููููุงุช:")
for word1, idx1 in vocab.items():
    similarities = []
    for word2, idx2 in vocab.items():
        if idx1 != idx2:
            sim = torch.cosine_similarity(
                embeddings[idx1], 
                embeddings[idx2], 
                dim=0
            )
            similarities.append((word2, sim.item()))
    
    # ุฃูุฑุจ 3 ูููุงุช
    similarities.sort(key=lambda x: x[1], reverse=True)
    closest = [w for w, _ in similarities[:3]]
    print(f"{word1} โ {', '.join(closest)}")
ุงูุฎูุงุตุฉ: ุงูุฃูุฒุงู ููุณุช ุนูุงูุงุช ุตุฑูุญุฉ ุจู ุชุถูููุงุช ูุฌุฑุฏุฉุ ูุงูุนูุงูุงุช ุชูุณุชูุชุฌ ุญูุณุจูุงูุ ูุง ุชูุฎุฒู ูุตูุงู.

ูู ุชุฑูุฏ ุฃู ุฃุนุทูู ูุซุงูุงู ุนูููุงู ุนูู ููููุฉ ุงุณุชุฎุฑุงุฌ ูุงุณุชูุดุงู ูุฐู ุงูุนูุงูุงุช ูู ูููุฐุฌ ุญููููุ

ูุนู
ููุชุงุฒ! ุณุฃุฑูู ุฎุทูุฉ ุจุฎุทูุฉ ููู ุชุณุชูุดู ุงูุนูุงูุงุช ุจูู ุงููููุงุช ูู ูููุฐุฌ ูุบูู ุญูููู.

ุงูุฌุฒุก 1: ุชุญููู ูููุฐุฌ ูุงุณุชูุดุงู ุงูุจููุฉ
python
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# ุฏุงูุฉ ุขููุฉ ูุชุญููู ุงููููุฐุฌ
def load_model_safely(model_path):
    """ุชุญููู ุงููููุฐุฌ ูุน ูุนุงูุฌุฉ ุงูุฃุฎุทุงุก"""
    try:
        checkpoint = torch.load(model_path, map_location='cpu')
        print(f"โ ุชู ุชุญููู ุงูููู ุจูุฌุงุญ")
        return checkpoint
    except Exception as e:
        print(f"โ ุฎุทุฃ ูู ุงูุชุญููู: {e}")
        return None

# ุงููุณุงุฑ - ุถุน ูุณุงุฑ ูููู ููุง
MODEL_PATH = "your_model.pt"  # ุบูุฑ ูุฐุง ูููุณุงุฑ ุงููุนูู

# ุชุญููู ุงููููุฐุฌ
checkpoint = load_model_safely(MODEL_PATH)

if checkpoint is None:
    # ูููุดุฆ ูููุฐุฌุงู ุชุฌุฑูุจูุงู ููุชูุถูุญ
    print("\n๐ง ุณููุดุฆ ูููุฐุฌุงู ุชุฌุฑูุจูุงู ููุชูุถูุญ...")
    
    # ุฅูุดุงุก ูููุฐุฌ ูุตุบุฑ
    vocab_size = 10000
    embedding_dim = 768
    
    # ูุญุงูุงุฉ ูุตูููุฉ ุงูุชุถููู
    embeddings = torch.randn(vocab_size, embedding_dim) * 0.1
    
    # ููุฑุฏุงุช ุนุฑุจูุฉ ูููุฐุฌูุฉ
    arabic_vocab = {
        0: "[PAD]", 1: "[UNK]", 2: "[CLS]", 3: "[SEP]", 4: "[MASK]",
        100: "ูุงู", 101: "ุตุฑู", 102: "ุฑุจุญ", 103: "ุฎุณุงุฑุฉ",
        104: "ุจูู", 105: "ุญุณุงุจ", 106: "ุนููุฉ", 107: "ุฏููุงุฑ",
        108: "ุฏููุงุฑ", 109: "ููุฑู", 110: "ุงุณุชุซูุงุฑ", 111: "ุณูู",
        112: "ุฃุณูู", 113: "ุณุนุฑ", 114: "ุดุฑุงุก", 115: "ุจูุน",
        116: "ุชุฌุงุฑุฉ", 117: "ุงูุชุตุงุฏ", 118: "ุซุฑูุฉ", 119: "ููุฑ",
        120: "ุณูุงุฑุฉ", 121: "ุจูุช", 122: "ุฃุฑุถ", 123: "ุฐูุจ",
        124: "ูุถุฉ", 125: "ููุท", 126: "ุบุงุฒ", 127: "ุทุงูุฉ",
        128: "ุดุฑูุฉ", 129: "ูุดุฑูุน", 130: "ุฑูุน", 131: "ุฏุฎู",
        132: "ุฑุงุชุจ", 133: "ุฃุฌุฑุฉ", 134: "ูุฑุถ", 135: "ูุงุฆุฏุฉ",
        136: "ุฑุจุง", 137: "ุฒูุงุฉ", 138: "ุตุฏูุฉ", 139: "ุชูููุฑ",
        140: "ุฅููุงู", 141: "ูุตุงุฑูู", 142: "ูุงุชูุฑุฉ", 143: "ุณุฏุงุฏ",
        144: "ุชุฃุฎูุฑ", 145: "ุบุฑุงูุฉ", 146: "ุนููุจุฉ", 147: "ูุฎุงููุฉ"
    }
    
    checkpoint = {
        'embeddings': embeddings,
        'vocab': arabic_vocab
    }
ุงูุฌุฒุก 2: ุฅูุฌุงุฏ ูุงุณุชุฎุฑุงุฌ ูุชุฌูุงุช ุงููููุงุช
python
def get_word_vector(checkpoint, target_word):
    """ุงุณุชุฎุฑุงุฌ ูุชุฌู ูููุฉ ูุญุฏุฏุฉ"""
    
    # ุงูุจุญุซ ูู ุฌููุน ุงูุฃูุงูู ุงููุญุชููุฉ ููุชุถูููุงุช
    embedding_keys = [
        'model.embeddings.word_embeddings.weight',
        'embeddings.weight', 
        'word_embeddings',
        'embedding.weight',
        'embeddings'
    ]
    
    embeddings_tensor = None
    for key in embedding_keys:
        if key in checkpoint:
            embeddings_tensor = checkpoint[key]
            print(f"๐ ูุฌุฏุช ุงูุชุถูููุงุช ูู: {key}")
            break
    
    # ุฅุฐุง ูู ูุฌุฏ ูู ุงููุณุชูู ุงูุฃููุ ุงุจุญุซ ุฃุนูู
    if embeddings_tensor is None and 'model_state_dict' in checkpoint:
        model_dict = checkpoint['model_state_dict']
        for key in embedding_keys:
            if key in model_dict:
                embeddings_tensor = model_dict[key]
                print(f"๐ ูุฌุฏุช ุงูุชุถูููุงุช ูู: model_state_dict['{key}']")
                break
    
    if embeddings_tensor is None:
        print("โ ูู ุฃุณุชุทุน ุฅูุฌุงุฏ ูุตูููุฉ ุงูุชุถููู")
        return None, None
    
    print(f"๐ ุดูู ูุตูููุฉ ุงูุชุถููู: {embeddings_tensor.shape}")
    
    # ุงูุจุญุซ ุนู ุงูููุฑุฏุงุช
    vocab = checkpoint.get('vocab', {})
    
    # ุฅุฐุง ูุงูุช ุงูููุฑุฏุงุช ูู {ูููุฉ: ูุนุฑู}
    if isinstance(vocab, dict) and target_word in vocab:
        word_id = vocab[target_word]
        print(f"๐ ุงููููุฉ '{target_word}' ููุง ุงููุนุฑู: {word_id}")
    
    # ุฅุฐุง ูุงูุช ุงูููุฑุฏุงุช ูู {ูุนุฑู: ูููุฉ}
    elif isinstance(vocab, dict):
        # ุงูุจุญุซ ุงูุนูุณู
        reverse_vocab = {v: k for k, v in vocab.items() if isinstance(k, str)}
        if target_word in reverse_vocab.values():
            word_id = [k for k, v in reverse_vocab.items() if v == target_word][0]
            print(f"๐ ุงููููุฉ '{target_word}' ููุง ุงููุนุฑู: {word_id}")
        else:
            print(f"โ๏ธ  ุงููููุฉ '{target_word}' ุบูุฑ ููุฌูุฏุฉ ูู ุงูููุฑุฏุงุช")
            print(f"   ุฌุฑุจ ุฅุญุฏู ูุฐู ุงููููุงุช: {list(reverse_vocab.values())[:10]}")
            return None, None
    else:
        print("โ ูุง ูููู ูุฑุงุกุฉ ุงูููุฑุฏุงุช")
        return None, None
    
    # ุงุณุชุฎุฑุงุฌ ุงููุชุฌู
    if word_id < embeddings_tensor.shape[0]:
        word_vector = embeddings_tensor[word_id]
        print(f"โ ูุชุฌู '{target_word}': {word_vector.shape}")
        return word_vector, word_id
    else:
        print(f"โ ุงููุนุฑู {word_id} ุฎุงุฑุฌ ูุทุงู ูุตูููุฉ ุงูุชุถููู")
        return None, None

# ุงุฎุชุจุงุฑ ูุน ูููุฉ "ูุงู"
target_word = "ูุงู"
word_vector, word_id = get_word_vector(checkpoint, target_word)

if word_vector is not None:
    print(f"\n๐ ุฃูู 20 ูููุฉ ูู ูุชุฌู '{target_word}':")
    print(word_vector[:20])
ุงูุฌุฒุก 3: ุงูุจุญุซ ุนู ุงููููุงุช ุงููุดุงุจูุฉ
python
def find_similar_words(checkpoint, target_word, top_n=10):
    """ุฅูุฌุงุฏ ุงููููุงุช ุงูุฃูุซุฑ ุชุดุงุจูุงู ููููุฉ ูุนููุฉ"""
    
    # ุงูุญุตูู ุนูู ูุชุฌู ุงููููุฉ ุงููุฏู
    target_vector, target_id = get_word_vector(checkpoint, target_word)
    
    if target_vector is None:
        return []
    
    # ุฅูุฌุงุฏ ูุตูููุฉ ุงูุชุถููู
    embeddings_tensor = None
    if 'embeddings' in checkpoint and torch.is_tensor(checkpoint['embeddings']):
        embeddings_tensor = checkpoint['embeddings']
    elif 'model_state_dict' in checkpoint:
        for key in checkpoint['model_state_dict']:
            if 'embed' in key.lower() and 'weight' in key.lower():
                embeddings_tensor = checkpoint['model_state_dict'][key]
                break
    
    if embeddings_tensor is None:
        print("โ ูุง ูููู ุฅูุฌุงุฏ ูุตูููุฉ ุงูุชุถููู")
        return []
    
    # ุงูุญุตูู ุนูู ุงูููุฑุฏุงุช
    vocab = checkpoint.get('vocab', {})
    
    # ุชุญููู ุงูููุฑุฏุงุช ุฅูู ูุงููุณ ูุนุฑูโูููุฉ
    idx_to_word = {}
    if isinstance(vocab, dict):
        for key, value in vocab.items():
            if isinstance(key, int) and isinstance(value, str):
                idx_to_word[key] = value
            elif isinstance(key, str) and isinstance(value, int):
                idx_to_word[value] = key
    
    print(f"\n๐ ุงูุจุญุซ ุนู ุงููููุงุช ุงููุดุงุจูุฉ ูู '{target_word}'...")
    
    # ุญุณุงุจ ุงูุชุดุงุจู ูุน ุฌููุน ุงููููุงุช
    similarities = []
    target_np = target_vector.numpy().reshape(1, -1)
    
    for idx in range(min(5000, len(embeddings_tensor))):  # ูุญุฏุฏ ููุณุฑุนุฉ
        if idx == target_id:
            continue
        
        if idx in idx_to_word:
            word_vec = embeddings_tensor[idx].numpy().reshape(1, -1)
            similarity = cosine_similarity(target_np, word_vec)[0][0]
            
            word_text = idx_to_word[idx]
            similarities.append((word_text, similarity, idx))
    
    # ุชุฑุชูุจ ุญุณุจ ุงูุชุดุงุจู
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\n๐ ุฃูุฑุจ {top_n} ูููุฉ ูู '{target_word}':")
    print("-" * 50)
    
    results = []
    for i, (word, sim, idx) in enumerate(similarities[:top_n]):
        print(f"{i+1:2}. {word:15} โ ุงูุชุดุงุจู: {sim:.4f} (ุงููุนุฑู: {idx})")
        results.append((word, sim))
    
    return results

# ุงุฎุชุจุงุฑ ุงูุจุญุซ
similar_words = find_similar_words(checkpoint, "ูุงู", top_n=15)
ุงูุฌุฒุก 4: ุชุตูุฑ ุงูุนูุงูุงุช ุจูู ุงููููุงุช
python
def visualize_word_relationships(checkpoint, words_list):
    """ุชุตูุฑ ุงูุนูุงูุงุช ุจูู ูุฌููุนุฉ ูู ุงููููุงุช"""
    
    print(f"\n๐จ ุชุตูุฑ ุงูุนูุงูุงุช ุจูู {len(words_list)} ูููุฉ...")
    
    # ุฌูุน ูุชุฌูุงุช ุงููููุงุช
    vectors = []
    valid_words = []
    
    for word in words_list:
        vector, _ = get_word_vector(checkpoint, word)
        if vector is not None:
            vectors.append(vector.numpy())
            valid_words.append(word)
    
    if len(vectors) < 2:
        print("โ ูุง ุชูุฌุฏ ูููุงุช ูุงููุฉ ููุชุตูุฑ")
        return
    
    vectors_array = np.array(vectors)
    print(f"๐ ุดูู ูุตูููุฉ ุงููุชุฌูุงุช: {vectors_array.shape}")
    
    # ุชูููู ุงูุฃุจุนุงุฏ ุจุงุณุชุฎุฏุงู PCA
    pca = PCA(n_components=2)
    reduced_vectors = pca.fit_transform(vectors_array)
    
    # ุฅูุดุงุก ุงูุฑุณู ุงูุจูุงูู
    plt.figure(figsize=(12, 10))
    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], alpha=0.6, s=100)
    
    # ุฅุถุงูุฉ ุชุณููุงุช
    for i, word in enumerate(valid_words):
        plt.annotate(
            word, 
            (reduced_vectors[i, 0], reduced_vectors[i, 1]),
            fontsize=12,
            fontname='Arial',
            alpha=0.8
        )
    
    # ุฑุณู ุฎุทูุท ุจูู ุงููููุงุช ุงููุชุดุงุจูุฉ
    for i in range(len(valid_words)):
        for j in range(i+1, len(valid_words)):
            # ุญุณุงุจ ุงููุณุงูุฉ
            dist = np.linalg.norm(reduced_vectors[i] - reduced_vectors[j])
            if dist < 1.0:  # ููุท ูููููุงุช ุงููุฑูุจุฉ
                plt.plot(
                    [reduced_vectors[i, 0], reduced_vectors[j, 0]],
                    [reduced_vectors[i, 1], reduced_vectors[j, 1]],
                    'gray', alpha=0.2, linewidth=0.5
                )
    
    plt.title(f'ุงูุนูุงูุงุช ุงูุฏูุงููุฉ ุจูู ุงููููุงุช', fontsize=16)
    plt.xlabel('ุงููููู ุงูุฑุฆูุณู 1', fontsize=14)
    plt.ylabel('ุงููููู ุงูุฑุฆูุณู 2', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # ุชูุณูุฑ ุงูููููุงุช
    print(f"\n๐ ุชูุณูุฑ PCA:")
    print(f"   ุงููููู 1 ููุณุฑ {pca.explained_variance_ratio_[0]*100:.1f}% ูู ุงูุชุจุงูู")
    print(f"   ุงููููู 2 ููุณุฑ {pca.explained_variance_ratio_[1]*100:.1f}% ูู ุงูุชุจุงูู")
    
    return reduced_vectors

# ุงุฎุชุจุงุฑ ุงูุชุตูุฑ
financial_words = ["ูุงู", "ุตุฑู", "ุฑุจุญ", "ุฎุณุงุฑุฉ", "ุจูู", "ุญุณุงุจ", 
                   "ุฏููุงุฑ", "ุฏููุงุฑ", "ุงุณุชุซูุงุฑ", "ุณูู", "ุฃุณูู"]

visualize_word_relationships(checkpoint, financial_words)
ุงูุฌุฒุก 5: ุชุญููู ุงูุนูููุงุช ุงูุฑูุงุถูุฉ ุนูู ุงููููุงุช
python
def word_arithmetic(checkpoint, positive_words, negative_words, top_n=5):
    """ุฅุฌุฑุงุก ุนูููุงุช ุฑูุงุถูุฉ ุนูู ุงููููุงุช (ูุดุงุจูุฉ: ููู - ุฑุฌู + ุงูุฑุฃุฉ = ูููุฉ)"""
    
    print(f"\n๐งฎ ุนูููุฉ ุญุณุงุจูุฉ ุนูู ุงููููุงุช:")
    print(f"   {' + '.join(positive_words)} - {' - '.join(negative_words)}")
    
    # ุฌูุน ูุชุฌูุงุช ุงููููุงุช ุงูุฅูุฌุงุจูุฉ
    positive_vectors = []
    for word in positive_words:
        vector, _ = get_word_vector(checkpoint, word)
        if vector is not None:
            positive_vectors.append(vector)
    
    # ุฌูุน ูุชุฌูุงุช ุงููููุงุช ุงูุณูุจูุฉ
    negative_vectors = []
    for word in negative_words:
        vector, _ = get_word_vector(checkpoint, word)
        if vector is not None:
            negative_vectors.append(vector)
    
    if not positive_vectors or not negative_vectors:
        print("โ ุจุนุถ ุงููููุงุช ุบูุฑ ููุฌูุฏุฉ")
        return
    
    # ุญุณุงุจ ุงููุชุฌู ุงููุงุชุฌ
    result_vector = sum(positive_vectors) - sum(negative_vectors)
    result_vector = result_vector / torch.norm(result_vector)  # ุชุทุจูุน
    
    print(f"โ ุชู ุญุณุงุจ ุงููุชุฌู ุงููุงุชุฌ")
    
    # ุงูุจุญุซ ุนู ุฃูุฑุจ ุงููููุงุช ูููุชุฌู ุงููุงุชุฌ
    embeddings_tensor = checkpoint.get('embeddings')
    if embeddings_tensor is None:
        return
    
    # ุชุญููู ุฅูู numpy ููุญุณุงุจ
    result_np = result_vector.numpy().reshape(1, -1)
    
    # ุญุณุงุจ ุงูุชุดุงุจู ูุน ุฌููุน ุงููููุงุช
    similarities = []
    vocab = checkpoint.get('vocab', {})
    
    # ุฅูุดุงุก ุฎุฑูุทุฉ ูุนุฑูโูููุฉ
    idx_to_word = {}
    if isinstance(vocab, dict):
        for k, v in vocab.items():
            if isinstance(k, int) and isinstance(v, str):
                idx_to_word[k] = v
            elif isinstance(k, str) and isinstance(v, int):
                idx_to_word[v] = k
    
    for idx in range(min(10000, len(embeddings_tensor))):
        if idx in idx_to_word:
            word_vec = embeddings_tensor[idx].numpy().reshape(1, -1)
            similarity = cosine_similarity(result_np, word_vec)[0][0]
            
            # ุชุฌูุจ ุงููููุงุช ุงููุฏุฎูุฉ
            word_text = idx_to_word[idx]
            if word_text not in positive_words + negative_words:
                similarities.append((word_text, similarity, idx))
    
    # ุชุฑุชูุจ ุงููุชุงุฆุฌ
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\n๐ฎ ุงููุชุงุฆุฌ ุงููุญุชููุฉ:")
    for i, (word, sim, idx) in enumerate(similarities[:top_n]):
        print(f"{i+1}. {word:15} โ ุงูุชุดุงุจู: {sim:.4f}")

# ูุซุงู ุนููู: ูุงู - ุตุฑู + ุงุณุชุซูุงุฑ = ุ
word_arithmetic(
    checkpoint, 
    positive_words=["ูุงู", "ุงุณุชุซูุงุฑ"], 
    negative_words=["ุตุฑู"],
    top_n=8
)
ุงูุฌุฒุก 6: ุญูุธ ูุงุณุชูุดุงู ุงููุชุงุฆุฌ
python
def export_word_analysis(checkpoint, central_word, output_file="word_analysis.txt"):
    """ุชุตุฏูุฑ ุชุญููู ูุงูู ููููุฉ ูุนููุฉ"""
    
    print(f"\n๐พ ุชุตุฏูุฑ ุชุญููู ุงููููุฉ '{central_word}'...")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"ุชุญููู ุงููููุฉ: {central_word}\n")
        f.write("=" * 50 + "\n\n")
        
        # 1. ูุนูููุงุช ุงููููุฉ ุงูุฃุณุงุณูุฉ
        vector, word_id = get_word_vector(checkpoint, central_word)
        if vector is not None:
            f.write(f"ุงููุนุฑู: {word_id}\n")
            f.write(f"ุทูู ุงููุชุฌู: {len(vector)}\n")
            f.write(f"ููู ุงููุชุฌู (10 ุฃููู): {vector[:10].tolist()}\n\n")
        
        # 2. ุงููููุงุช ุงููุดุงุจูุฉ
        f.write("ุงููููุงุช ุงูุฃูุซุฑ ุชุดุงุจููุง:\n")
        f.write("-" * 30 + "\n")
        
        similar_words = find_similar_words(checkpoint, central_word, top_n=20)
        for i, (word, sim) in enumerate(similar_words):
            f.write(f"{i+1:2}. {word:15} โ {sim:.4f}\n")
        
        # 3. ุชุญููู ุงููุฌููุนุงุช
        f.write("\n\nุชุญููู ุงููุฌููุนุงุช ุงูุฏูุงููุฉ:\n")
        f.write("=" * 50 + "\n")
        
        # ูุฌููุนุงุช ููุชุญููู
        categories = {
            "ุงููุงู ูุงูุงูุชุตุงุฏ": ["ูุงู", "ุตุฑู", "ุฑุจุญ", "ุฎุณุงุฑุฉ", "ุจูู", "ุญุณุงุจ"],
            "ุงูุนููุงุช": ["ุฏููุงุฑ", "ุฏููุงุฑ", "ููุฑู", "ุนููุฉ"],
            "ุงูุงุณุชุซูุงุฑ": ["ุงุณุชุซูุงุฑ", "ุณูู", "ุฃุณูู", "ุฑูุน"],
            "ุงููุนุงููุงุช": ["ุดุฑุงุก", "ุจูุน", "ุณุนุฑ", "ูุงุชูุฑุฉ"]
        }
        
        for category, words in categories.items():
            f.write(f"\n{category}:\n")
            for word in words:
                if word != central_word:
                    vec, _ = get_word_vector(checkpoint, word)
                    if vec is not None and vector is not None:
                        sim = cosine_similarity(
                            vector.numpy().reshape(1, -1),
                            vec.numpy().reshape(1, -1)
                        )[0][0]
                        f.write(f"  - {word}: {sim:.4f}\n")
    
    print(f"โ ุชู ุงูุชุตุฏูุฑ ุฅูู: {output_file}")

# ุชุตุฏูุฑ ุชุญููู ูููุฉ "ูุงู"
export_word_analysis(checkpoint, "ูุงู", "ูุงู_analysis.txt")
ุงูุฌุฒุก 7: ููููุฉ ุชุดุบูู ูุฐุง ุนูู ูููุฐุฌู ุงููุนูู
python
# 1. ุฃููุงู: ุงูุชุดุงู ูููู ูููู
def explore_checkpoint_structure(checkpoint):
    """ุงุณุชูุดุงู ูููู ููู .pt"""
    
    print("๐ ุงุณุชูุดุงู ูููู ุงูููู:")
    print("=" * 60)
    
    def recursive_explore(obj, indent=0, max_items=5):
        indent_str = "  " * indent
        
        if isinstance(obj, dict):
            print(f"{indent_str}๐ ูุงููุณ ({len(obj)} ุนูุตุฑ):")
            for i, (key, value) in enumerate(obj.items()):
                if i < max_items:
                    print(f"{indent_str}  โโ '{key}': {type(value).__name__}")
                    if isinstance(value, (dict, list, tuple)) and indent < 2:
                        recursive_explore(value, indent + 2, 3)
                else:
                    print(f"{indent_str}  โโ ... ูุขุฎุฑูู")
                    break
                    
        elif torch.is_tensor(obj):
            print(f"{indent_str}๐ฆ ุชูุณูุฑ: {obj.shape} | {obj.dtype}")
            
        elif isinstance(obj, (list, tuple)):
            print(f"{indent_str}๐ {'ูุงุฆูุฉ' if isinstance(obj, list) else 'ูุฌููุนุฉ'} ({len(obj)} ุนูุตุฑ):")
            for i, item in enumerate(obj[:min(3, len(obj))]):
                print(f"{indent_str}  โโ [{i}]: {type(item).__name__}")
            
        else:
            print(f"{indent_str}๐ {type(obj).__name__}: {str(obj)[:50]}...")
    
    recursive_explore(checkpoint)

# ุชุดุบูู ุงูุงุณุชูุดุงู
explore_checkpoint_structure(checkpoint)
ูุตุงุฆุญ ุนูููุฉ:
ุงุจุฏุฃ ุจุงูุชุดุงู ุงููููู ุจุงุณุชุฎุฏุงู explore_checkpoint_structure()

ุงุจุญุซ ุนู word_embeddings ุฃู embedding.weight

ุงุจุญุซ ุนู vocab ุฃู tokenizer.get_vocab()

ุฌุฑุจ ูููุงุช ุดุงุฆุนุฉ ุฃููุงู: "ูู"ุ "ูู"ุ "ุนูู"ุ "ุฅูู"

ุงุณุชุฎุฏู PCA ูููู ุงููุฌููุนุงุช ุงูุฏูุงููุฉ

ูุซุงู ุนูู ูุชุงุฆุฌ ูุชููุนุฉ:
text
๐ ุงููููุงุช ุงููุดุงุจูุฉ ูู "ูุงู":
1. ุตุฑู           โ 0.8923
2. ุญุณุงุจ          โ 0.8765  
3. ุจูู           โ 0.8541
4. ุฑุจุญ           โ 0.8321
5. ุฏููุงุฑ         โ 0.8214
6. ุงุณุชุซูุงุฑ       โ 0.8123
7. ุนููุฉ          โ 0.8012
8. ุฎุณุงุฑุฉ         โ 0.7890  # ูุซูุฑุฉ ููุงูุชูุงู!
9. ุณูู           โ 0.7765
10. ุฏููุงุฑ        โ 0.7654
ุงูุฎูุงุตุฉ: ุงูุนูุงูุงุช ุบูุฑ ุตุฑูุญุฉ ูู ุงููููุ ููููุง ูุงุจูุฉ ููุงุณุชุฎุฑุงุฌ ูู ุฎูุงู ุงูุนูููุงุช ุงูุฑูุงุถูุฉ ุนูู ุงููุชุฌูุงุช.

ูู ุชุฑูุฏ ุฃู ุฃุดุฑุญ ููููุฉ ุชุนุฏูู ูุฐู ุงููุชุฌูุงุช ุฃู ุชุฏุฑูุจ ุชุถูููุงุช ุฎุงุตุฉ ุจูุ

ุงูุง ุจุตุฑุงุญุฉ ุงุณุฃู ููุฐุง ุงุณุฆูุฉ ููู ุงููู ุจุดูู ุฌูุฏ ุซู ุฃุณูู ูู ุงุจุฏุงุน ูุชุทููุฑ ุฌุฏูุฏ ููููุงุฐุฌ ุงููุบููุฉ. ุงูุง ุงุฑูุฏ ุงููุตูู ุงูู ููู ูููุฐุฌ ูุบูู ุฏููุงูููู ูููู ููุชุทูุฑ ูุชุชุบูุฑ ุงูุฒุงูู ูุชุณุชุญุฏุซ ุงูุนูุงูุงุช ุจูููุง ุจูู ูุชุฑุฉ ูุงุฎุฑู. ุงุฑูุฏ ูููุฐุฌ ูุบูู ูุณุชูุชุฌ ุนูุงูุงุช ุฌุฏูุฏุฉ ูู ุฎุตุงุฆุต ูุนูุงูุงุช ุงุฎุฑู. ูุซูุง ุงุญูุฏ ุงูุณุง. ุฎุงูุฏ ุงูุณุงู. ุนูุฑ ุงูุณุงู. ุฒูุฏ ุงูุณุงู.. ููุฌุฏ ุงู ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ ูุฒูุฏ ูุญุจ ุงูุชูุงุญ ูุนูุฑ ูุญุจ ุงูุชูุงุญ ููู ูุณุชูุชุฌ ุงูู ุจุงูุนููู ุงูุงูุณุงู ูุญุจ ุงูุชูุงุญ ููุฐูู ุนูุณ ูุฐู ุงูุนูุงูุฉ. ูุซูุง ููุชูุจ ูุฏูู ุงู ุงูุงูุณุงู ูููุฑ ููู ูุณุชูุชุฌ ุจุงูุนููู ุงู ุนูุฑ ูููุฑุ ูููุฐุง. ูุฑูุฏ ููู ูููุฐุฌ ูุบูู ูุงุจู ููุชูุณุน ูุงูู ููู txt ุงู ููู .py ุญูุซ ูููู ูุจุงูุซูู ุงู ููุชุจ ูููุณู ููุฏ ูู ูููู ููู ูู ุญุงูุฉ ุชูููุฐ. ูุญู ูุฑูุฏ ุงู ุงูููู ูุง ููุชุจ ุงูุฒุงู ุนุฏุฏูุฉ ุจุตูุฑุฉ ูุจุงุดุฑุฉ ูููู ุจุฏู ุฐูู ููุฌุฏ ููุฏ ูููู ููุจุฑูุงูุฌ ุงุซูุงุก ุงูุชุฏุฑูุจ ุงู ุงุซูุงุก ุงูุชูููุฐ ุงู ููุชุจ ููู ุงูุนุฏุฏ ุงูุฐู ูุฑุงู ููุงุณุจุง ูููู ุฎุงูุชู ุฒูุงุฏุฉ ูููุตุงู ููุชุนุฏูู ุงููุณุชูุฑ ูุงู ุงููููุฐุฌ ูุนุชูุฏ ุนูู ููุน ุฌุฏูุฏ ูู ุงููููุงุช ุงูุชูุณุนูุฉ ูููุณ ูู ููุน .pt ุงู ูู ุงูููุน ุงููุถุบูุท ูุบูุฑ ุฐูู
ูุง ุชุตูู ูู ุซูุฑุฉ ูู ุงูููุงุฐุฌ ุงููุบููุฉ! ุฃูุช ุชุชุญุฏุซ ุนู ูููุฐุฌ ุนุตุจู ุฑูุฒู ูุฌูู (Neuro-Symbolic AI) ูุน ุฐุงูุฑุฉ ุฏููุงููููุฉ ูุงุจูุฉ ููููู. ูุฐู ููุฑุฉ ูุชูุฏูุฉ ุฌุฏุงู ูุณุฃุดุฑุญ ููู ูููู ุชุญููููุง.

1. ุงูููููู ุงูุฌุฏูุฏ: ุงููููุฐุฌ ุงููุบูู ุงูุฏููุงูููู ุงูุฐุงุชู ุงูููู
ุงููุดููุฉ ุงูุญุงููุฉ ูู ูููุงุช .pt:
ุซุงุจุชุฉ ุจุนุฏ ุงูุชุฏุฑูุจ

ุบูุฑ ูุงุจูุฉ ูููุฑุงุกุฉ ุงูุจุดุฑูุฉ

ูุง ุชุณุชูุนุจ ูุนุฑูุฉ ุฌุฏูุฏุฉ ุฏูู ุฅุนุงุฏุฉ ุชุฏุฑูุจ ูุงููุฉ

ุนูุงูุงุช ูุถููุฉ ูุบูุฑ ูุงุจูุฉ ููุชูุณูุฑ

ุงูุญู ุงูููุชุฑุญ: ูุธุงู ูุฌูู
text
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ         ุงููููุฐุฌ ุงูุฏููุงูููู ุงูุฐุงุชู ุงูููู        โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ 1. ูุงุนุฏุฉ ูุนุฑูุฉ ุฑูุฒูุฉ (Symbolic KB) โ JSON/YAML โ
โ 2. ุดุจูุฉ ุนุตุจูุฉ ุฏููุงููููุฉ โ ุฃุฌุฒุงุก .py ูุงุจูุฉ ููุชุนุฏููโ
โ 3. ูุญุฑู ุงุณุชุฏูุงู โ ูุฑุจุท ุจูู ุงูุงุซููู              โ
โ 4. ุฐุงูุฑุฉ ุทูููุฉ ุงููุฏู โ ูุงุนุฏุฉ ุจูุงูุงุช ูุนุฑูุฉ      โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
2. ุชุตููู ูุธุงู ุงููููุงุช ุงูุฏููุงูููู
ุฃ. ููู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุฑูุฒูุฉ (knowledge.kb)
yaml
# ูุงุนุฏุฉ ูุนุฑูุฉ ูุงุจูุฉ ูููุฑุงุกุฉ ูุงููุชุงุจุฉ
concepts:
  human:
    properties: ["thinks", "eats", "sleeps"]
    instances: ["ahmed", "khalid", "omar", "zayed"]
    relations:
      loves: ["apple", "knowledge", "family"]
  
  apple:
    properties: ["fruit", "red", "sweet"]
    relations:
      loved_by: ["human"]

facts:
  - "ahmed is human"
  - "khalid is human" 
  - "omar is human"
  - "zayed is human"
  - "khalid loves apple"
  - "zayed loves apple"
  - "omar loves apple"

rules:
  - "IF X is human AND Y loves apple THEN human loves apple"
  - "IF human has property P THEN instances of human have property P"

# ุฅุญุตุงุฆูุงุช ุฏููุงููููุฉ
statistics:
  human_loves_apple:
    evidence_count: 3
    total_humans: 4
    confidence: 0.75
    last_updated: "2024-01-15 10:30:00"
ุจ. ููู ุงูุชุนูู ุงูุฏููุงูููู (dynamic_model.py)
python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ูููุฐุฌ ูุบูู ุฏููุงูููู ุฐุงุชู ุงูุชุนุฏูู
ูููู ููุจุฑูุงูุฌ ูุชุงุจุฉ ููุฏ ููู ุฃุซูุงุก ุงูุชุดุบูู
"""

import ast
import inspect
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Callable

class DynamicNeuron:
    """ุฎููุฉ ุนุตุจูุฉ ุฏููุงููููุฉ ูุงุจูุฉ ููุชุนุฏูู"""
    
    def __init__(self, neuron_id: str, initial_weight: float = 0.0):
        self.id = neuron_id
        self.weight = initial_weight
        self.connections = {}  # {neuron_id: strength}
        self.activation_history = []
        self.meta = {
            "created": datetime.now().isoformat(),
            "modified": datetime.now().isoformat(),
            "type": "generic"
        }
    
    def to_code(self) -> str:
        """ุชุญููู ุงูุฎููุฉ ุฅูู ููุฏ ุจุงูุซูู ูููู ุชูููุฐู"""
        return f"""
# Neuron: {self.id}
neuron_{self.id} = DynamicNeuron(
    neuron_id='{self.id}',
    initial_weight={self.weight}
)
neuron_{self.id}.connections = {json.dumps(self.connections)}
neuron_{self.id}.meta = {json.dumps(self.meta, indent=2)}
"""
    
    def update_from_code(self, code: str):
        """ุชุญุฏูุซ ุงูุฎููุฉ ูู ููุฏ ุจุงูุซูู"""
        # ุชูููุฐ ุงูููุฏ ูู ุณูุงู ูุญูู
        local_vars = {}
        exec(code, globals(), local_vars)
        
        # ุชุญุฏูุซ ุงูููู
        neuron_key = f"neuron_{self.id}"
        if neuron_key in local_vars:
            updated = local_vars[neuron_key]
            self.weight = updated.weight
            self.connections = updated.connections
            self.meta = updated.meta
            self.meta["modified"] = datetime.now().isoformat()

class SelfModifyingModel:
    """ูููุฐุฌ ููููู ุชุนุฏูู ููุฏ ูุตุฏุฑู ุงูุฎุงุต"""
    
    def __init__(self, model_file: str = "dynamic_model.py"):
        self.model_file = model_file
        self.neurons: Dict[str, DynamicNeuron] = {}
        self.knowledge_base = {}
        self.inference_rules = []
        
        # ุชุญููู ุงูููุฏ ุงูุญุงูู
        self.load_current_code()
    
    def load_current_code(self):
        """ุชุญููู ุงูููุฏ ุงูุญุงูู ูู ุงูููู"""
        with open(self.model_file, 'r', encoding='utf-8') as f:
            self.source_code = f.read()
        
        # ุงุณุชุฎุฑุงุฌ ุชุนุฑููุงุช ุงูุฎูุงูุง ุงูุนุตุจูุฉ
        self.extract_neurons_from_code()
    
    def extract_neurons_from_code(self):
        """ุงุณุชุฎุฑุงุฌ ุงูุฎูุงูุง ุงูุนุตุจูุฉ ูู ุงูููุฏ ุงููุตุฏุฑู"""
        tree = ast.parse(self.source_code)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        if target.id.startswith('neuron_'):
                            # ุชูููุฐ ูุฐุง ุงูุฌุฒุก ูู ุงูููุฏ ูุงุณุชุฎุฑุงุฌ ุงูุฎููุฉ
                            try:
                                local_vars = {}
                                exec(compile(ast.Module([node], type_ignores=[]), 
                                           '<string>', 'exec'), globals(), local_vars)
                                
                                neuron_obj = local_vars.get(target.id)
                                if isinstance(neuron_obj, DynamicNeuron):
                                    self.neurons[neuron_obj.id] = neuron_obj
                            except:
                                pass
    
    def learn_pattern(self, pattern: Dict[str, Any]):
        """ุชุนูู ููุท ุฌุฏูุฏ ูุฅุถุงูุชู ุฏููุงููููุงู ุฅูู ุงูููุฏ"""
        
        # 1. ุฅูุดุงุก ุฎูุงูุง ุนุตุจูุฉ ุฌุฏูุฏุฉ ููุฃููุงุท
        new_neurons = []
        for key, value in pattern.items():
            neuron_id = f"pattern_{hash(str(pattern))}_{key}"
            
            if neuron_id not in self.neurons:
                neuron = DynamicNeuron(neuron_id)
                neuron.weight = float(value) if isinstance(value, (int, float)) else 0.5
                neuron.meta["type"] = "pattern_cell"
                neuron.meta["pattern"] = str(pattern)
                
                self.neurons[neuron_id] = neuron
                new_neurons.append(neuron)
        
        # 2. ุฅูุดุงุก ูุงุนุฏุฉ ุงุณุชุฏูุงู ุฌุฏูุฏุฉ
        rule_code = self.generate_inference_rule(pattern)
        self.add_rule_to_source(rule_code)
        
        # 3. ุชุญุฏูุซ ููู ุงููููุฐุฌ
        self.update_model_file()
        
        return new_neurons
    
    def generate_inference_rule(self, pattern: Dict[str, Any]) -> str:
        """ุฅูุดุงุก ูุงุนุฏุฉ ุงุณุชุฏูุงู ุฌุฏูุฏุฉ ูููุฏ ุจุงูุซูู"""
        pattern_hash = hash(str(pattern))
        
        rule = f"""
# Rule generated at {datetime.now().isoformat()}
def rule_{pattern_hash}(context):
    \"\"\"ุชููุงุฆูุงู: {pattern}\"\"\"
    
    # ุชุญูู ูู ุงูุฃููุงุท
    conditions_met = True
    {self.generate_condition_checks(pattern)}
    
    if conditions_met:
        # ุชูููุฐ ุงูุงุณุชุฏูุงู
        {self.generate_inference_action(pattern)}
        
        # ุชุญุฏูุซ ุงููุนุฑูุฉ
        knowledge_entry = {{
            "pattern": {pattern},
            "inferred_at": "{datetime.now().isoformat()}",
            "confidence": self.calculate_confidence(pattern, context)
        }}
        
        if "inferences" not in self.knowledge_base:
            self.knowledge_base["inferences"] = []
        self.knowledge_base["inferences"].append(knowledge_entry)
        
        # ุฅูุดุงุก ุฎูุงูุง ุนุตุจูุฉ ุฌุฏูุฏุฉ ููุนูุงูุฉ ุงูููุชุดูุฉ
        self.create_neurons_for_relation(pattern, context)
        
        return True
    return False

# ุฅุถุงูุฉ ุงููุงุนุฏุฉ ุฅูู ูุญุฑู ุงูุงุณุชุฏูุงู
self.inference_rules.append(rule_{pattern_hash})
"""
        return rule
    
    def update_model_file(self):
        """ุชุญุฏูุซ ููู ุงููููุฐุฌ ุจุฅุถุงูุฉ ุงูููุฏ ุงูุฌุฏูุฏ"""
        
        # 1. ุฅุถุงูุฉ ุงูุฎูุงูุง ุงูุนุตุจูุฉ ุงูุฌุฏูุฏุฉ
        new_code = "\n\n# === DYNAMICALLY ADDED NEURONS ===\n"
        for neuron_id, neuron in self.neurons.items():
            if "pattern_cell" in neuron.meta.get("type", ""):
                new_code += neuron.to_code() + "\n"
        
        # 2. ุฅูุฌุงุฏ ููุงู ุงูุฅุถุงูุฉ ูู ุงูููู
        lines = self.source_code.split('\n')
        insertion_point = len(lines)
        
        for i, line in enumerate(lines):
            if "# END OF STATIC CODE" in line:
                insertion_point = i
                break
        
        # 3. ุฅุฏุฎุงู ุงูููุฏ ุงูุฌุฏูุฏ
        lines.insert(insertion_point, new_code)
        updated_code = '\n'.join(lines)
        
        # 4. ูุชุงุจุฉ ุงูููู ุงููุญุฏุซ
        with open(self.model_file, 'w', encoding='utf-8') as f:
            f.write(updated_code)
        
        # 5. ุฅุนุงุฏุฉ ุชุญููู ุงูููุฏ
        self.load_current_code()
        
        print(f"โ ุชู ุชุญุฏูุซ ุงูููู {self.model_file} ุฏููุงููููุงู")

# === INITIAL MODEL CODE ===
model = SelfModifyingModel()

# END OF STATIC CODE
# ุจุนุฏ ูุฐุง ุงูุณุทุฑ ุณููุชุจ ุงููููุฐุฌ ููุฏุงู ุชููุงุฆูุงู
ุฌ. ูุญุฑู ุงูุงุณุชุฏูุงู ุงูุฏููุงูููู (inference_engine.py)
python
class DynamicInferenceEngine:
    """ูุญุฑู ุงุณุชุฏูุงู ูุชุนูู ูููุชุจ ููุงุนุฏู ุจููุณู"""
    
    def __init__(self):
        self.rules = []
        self.learned_patterns = []
        self.rule_counter = 0
        
        # ูุงุนุฏุฉ ูุนุฑูุฉ ุฏููุงููููุฉ
        self.knowledge_graph = {
            "entities": {},
            "relations": {},
            "properties": {},
            "rules": []
        }
    
    def learn_from_examples(self, examples: List[Dict]):
        """ุงูุชุนูู ูู ุฃูุซูุฉ ูุฅูุดุงุก ููุงุนุฏ ุฌุฏูุฏุฉ"""
        
        for example in examples:
            # ุงุณุชุฎุฑุงุฌ ุงูุฃููุงุท
            patterns = self.extract_patterns(example)
            
            for pattern in patterns:
                # ุชุญูู ุฅุฐุง ูุงู ุงูููุท ุฌุฏูุฏุงู
                if not self.pattern_exists(pattern):
                    # ุฅูุดุงุก ูุงุนุฏุฉ ุฌุฏูุฏุฉ
                    new_rule = self.create_rule_from_pattern(pattern)
                    
                    # ุฅุถุงูุฉ ุงููุงุนุฏุฉ
                    self.rules.append(new_rule)
                    self.learned_patterns.append(pattern)
                    
                    # ูุชุงุจุฉ ุงููุงุนุฏุฉ ุฅูู ููู
                    self.write_rule_to_file(new_rule)
                    
                    print(f"๐ ุชุนููุช ูุงุนุฏุฉ ุฌุฏูุฏุฉ: {new_rule['description']}")
    
    def create_rule_from_pattern(self, pattern: Dict) -> Dict:
        """ุฅูุดุงุก ูุงุนุฏุฉ ุงุณุชุฏูุงู ูู ููุท"""
        self.rule_counter += 1
        
        rule_id = f"auto_rule_{self.rule_counter:04d}"
        
        # ุฅูุดุงุก ููุฏ ุงููุงุนุฏุฉ ุฏููุงููููุงู
        rule_code = f"""
def {rule_id}(entities):
    \"\"\"ูุงุนุฏุฉ ุชููุงุฆูุฉ: {pattern}\"\"\"
    
    # ุงูุดุฑูุท
    conditions = {self.pattern_to_conditions(pattern)}
    
    # ุงูุฅุฌุฑุงุก
    if all(conditions):
        # ุงูุงุณุชุฏูุงู
        inference = {self.pattern_to_inference(pattern)}
        
        # ุฅุถุงูุฉ ุฅูู ุงููุนุฑูุฉ
        self.add_to_knowledge(inference)
        
        # ุฅูุดุงุก ุนูุงูุงุช ุฌุฏูุฏุฉ
        self.create_new_relations(inference)
        
        return True
    return False
"""
        
        # ุชูููุฐ ุงูููุฑ ูุฅูุดุงุก ุงูุฏุงูุฉ
        local_vars = {}
        exec(rule_code, globals(), local_vars)
        
        rule_function = local_vars[rule_id]
        
        return {
            "id": rule_id,
            "function": rule_function,
            "pattern": pattern,
            "code": rule_code,
            "created": datetime.now().isoformat(),
            "description": self.describe_pattern(pattern)
        }
    
    def write_rule_to_file(self, rule: Dict):
        """ูุชุงุจุฉ ุงููุงุนุฏุฉ ุฅูู ููู ูุตุฏุฑู"""
        
        filename = "auto_generated_rules.py"
        
        # ูุฑุงุกุฉ ุงูููู ุงูุญุงูู
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                existing_content = f.read()
        except FileNotFoundError:
            existing_content = "# ุงูููุงุนุฏ ุงูุชููุงุฆูุฉ ุงููุชููุฏุฉ\n\n"
        
        # ุฅุถุงูุฉ ุงููุงุนุฏุฉ ุงูุฌุฏูุฏุฉ
        new_content = existing_content + "\n\n" + rule["code"]
        
        # ูุชุงุจุฉ ุงูููู ุงููุญุฏุซ
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        # ุชุญุฏูุซ ูุงุฆูุฉ ุงูููุงุนุฏ ูู ููู ุงูุชููุฆุฉ
        self.update_rule_registry(rule)
    
    def update_rule_registry(self, rule: Dict):
        """ุชุญุฏูุซ ุณุฌู ุงูููุงุนุฏ"""
        registry_file = "rule_registry.json"
        
        try:
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
        except FileNotFoundError:
            registry = {"rules": []}
        
        registry["rules"].append({
            "id": rule["id"],
            "created": rule["created"],
            "pattern": rule["pattern"],
            "description": rule["description"]
        })
        
        with open(registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry, f, indent=2, ensure_ascii=False)
    
    def pattern_to_conditions(self, pattern: Dict) -> List[str]:
        """ุชุญููู ุงูููุท ุฅูู ุดุฑูุท ุชูููุฐูุฉ"""
        conditions = []
        
        if "if" in pattern:
            for condition in pattern["if"]:
                conditions.append(f"'{condition}' in entities")
        
        return conditions
    
    def pattern_to_inference(self, pattern: Dict) -> Dict:
        """ุชุญููู ุงูููุท ุฅูู ุงุณุชุฏูุงู"""
        inference = {"then": pattern.get("then", {})}
        
        # ุฅุถุงูุฉ ุฅุญุตุงุฆูุงุช
        inference["metadata"] = {
            "confidence": 0.8,  # ุณูุชุนูู ูุฐุง ูู ุงูุจูุงูุงุช
            "support_count": 1,
            "last_used": datetime.now().isoformat()
        }
        
        return inference
    
    def describe_pattern(self, pattern: Dict) -> str:
        """ูุตู ุงูููุท ุจูุบุฉ ุทุจูุนูุฉ"""
        if "if" in pattern and "then" in pattern:
            conditions = " ู ".join(pattern["if"])
            conclusions = " ู ".join(pattern["then"])
            return f"ุฅุฐุง {conditions} ูุฅู {conclusions}"
        return str(pattern)
3. ูุธุงู ุงููููุงุช ุงููุงูู
text
my_dynamic_model/
โโโ ๐ core/
โ   โโโ ๐ dynamic_model.py          # ุงููููุฐุฌ ุงูุฑุฆูุณู (ูุชุนุฏู ููุณู)
โ   โโโ ๐ง inference_engine.py       # ูุญุฑู ุงูุงุณุชุฏูุงู
โ   โโโ ๐ self_modifier.py          # ูุนุฏู ุงูุฐุงุชู
โโโ ๐ knowledge/
โ   โโโ ๐ knowledge.kb              # ูุงุนุฏุฉ ูุนุฑูุฉ (YAML/JSON)
โ   โโโ ๐ statistics.json           # ุฅุญุตุงุฆูุงุช ุงูุชุนูู
โ   โโโ ๐ patterns.db               # ูุงุนุฏุฉ ุจูุงูุงุช ุงูุฃููุงุท
โโโ ๐ auto_generated/
โ   โโโ ๐ฏ rules/                    # ุงูููุงุนุฏ ุงููููุฏุฉ ุชููุงุฆูุงู
โ   โโโ ๐งฌ neurons/                  # ุงูุฎูุงูุง ุงูุนุตุจูุฉ ุงููููุฏุฉ
โ   โโโ ๐ relations/                # ุงูุนูุงูุงุช ุงูุฌุฏูุฏุฉ
โโโ ๐ memory/
โ   โโโ ๐ episodic_memory.json      # ุฐุงูุฑุฉ ุงูุฃุญุฏุงุซ
โ   โโโ ๐งพ semantic_memory.json      # ุฐุงูุฑุฉ ุงููุนุงูู
โ   โโโ ๐ working_memory.json       # ุงูุฐุงูุฑุฉ ุงูุนุงููุฉ
โโโ ๐ config.yaml                   # ุฅุนุฏุงุฏุงุช ุงููููุฐุฌ
4. ูุซุงู ุนููู: ููู ูุชุนูู ุงููููุฐุฌ ูููุชุจ ููุฏุงู ูููุณู
python
# ูููุฐุฌ ุชุนูู ุงูุนูุงูุฉ: ุงูุฅูุณุงู ูุญุจ ุงูุชูุงุญ
learning_examples = [
    {"ุฅุฐุง": ["ุฎุงูุฏ ุฅูุณุงู", "ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ"], "ูุฅู": ["ุงูุฅูุณุงู ูุญุจ ุงูุชูุงุญ"]},
    {"ุฅุฐุง": ["ุฒูุฏ ุฅูุณุงู", "ุฒูุฏ ูุญุจ ุงูุชูุงุญ"], "ูุฅู": ["ุงูุฅูุณุงู ูุญุจ ุงูุชูุงุญ"]},
    {"ุฅุฐุง": ["ุนูุฑ ุฅูุณุงู", "ุนูุฑ ูุญุจ ุงูุชูุงุญ"], "ูุฅู": ["ุงูุฅูุณุงู ูุญุจ ุงูุชูุงุญ"]}
]

# ุงููุญุฑู ูุชุนูู ูููุชุจ ููุงุนุฏ ุฌุฏูุฏุฉ
engine = DynamicInferenceEngine()
engine.learn_from_examples(learning_examples)

# ุงููุงุชุฌ ูู auto_generated_rules.py ุณูููู:
"""
def auto_rule_0001(entities):
    '''ูุงุนุฏุฉ ุชููุงุฆูุฉ: {'ุฅุฐุง': ['ุฎุงูุฏ ุฅูุณุงู', 'ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ'], 'ูุฅู': ['ุงูุฅูุณุงู ูุญุจ ุงูุชูุงุญ']}'''
    
    conditions = [
        "'ุฎุงูุฏ ุฅูุณุงู' in entities",
        "'ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ' in entities"
    ]
    
    if all(conditions):
        inference = {
            "then": ["ุงูุฅูุณุงู ูุญุจ ุงูุชูุงุญ"],
            "metadata": {
                "confidence": 0.8,
                "support_count": 1,
                "last_used": "2024-01-15T10:30:00"
            }
        }
        
        self.add_to_knowledge(inference)
        self.create_new_relations(inference)
        
        return True
    return False
"""

# ุซู ูุณุชุฎุฏู ูุฐู ุงููุงุนุฏุฉ ููุงุณุชุฏูุงู ุนูู ุฃูุซูุฉ ุฌุฏูุฏุฉ:
result = engine.apply_rules(["ุฃุญูุฏ ุฅูุณุงู"])
# ุณููุชุฌ: "ุงูุฅูุณุงู ูุญุจ ุงูุชูุงุญ" ุจุซูุฉ ูุนููุฉ
5. ุฎุตุงุฆุต ูุฐุง ุงููุธุงู ุงูุฌุฏูุฏ:
โ ุงููููุฒุงุช:
ูุงุจู ูููุฑุงุกุฉ ุงูุจุดุฑูุฉ โ ูููุงุช YAML/JSON/Python

ุฏููุงูููู โ ูููู ุฃุซูุงุก ุงูุชุดุบูู

ุชูุณูุฑู โ ุงูุนูุงูุงุช ูุงุถุญุฉ ูููุณุช ูุถููุฉ

ุฐุงุชู ุงูุชุนุฏูู โ ููุชุจ ููุฏุงู ูููุณู

ูุงุจู ููุชูุณุน โ ูุถูู ูููุงุช ูุนูุงูุงุช ุฌุฏูุฏุฉ

โก ุชุญุฏูุงุช ูุญููู:
ุงูุณูุงูุฉ โ ูุณุฎ ุงุญุชูุงุทูุฉ ุชููุงุฆูุฉ

ุงูุชูุงุณู โ ุชุญูู ูู ุงูุชูุงุณู ูุจู ุงููุชุงุจุฉ

ุงูุฃุฏุงุก โ ุชุฎุฒูู ูุคูุช ููููุฏ ุงููุชุฑุฌู

ุงูุฃูุงู โ ุชุฃููู ุงููุชุงุจุฉ ุงูุฐุงุชูุฉ

6. ุฎุทูุงุช ุงูุชูููุฐ:
python
# 1. ุฅูุดุงุก ุงููููุฐุฌ ุงูุฏููุงูููู
model = SelfModifyingModel("my_dynamic_model.py")

# 2. ุชุนูู ูู ุจูุงูุงุช
model.learn_from_data([
    ("ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ", {"ุฎุงูุฏ": "ุฅูุณุงู", "ุงูุชูุงุญ": "ูุงููุฉ"}),
    ("ุฒูุฏ ูุญุจ ุงูุชูุงุญ", {"ุฒูุฏ": "ุฅูุณุงู", "ุงูุชูุงุญ": "ูุงููุฉ"})
])

# 3. ุงูุงุณุชุฏูุงู ุนูู ูุนูููุงุช ุฌุฏูุฏุฉ
inference = model.infer("ุนูุฑ ูุญุจ ุงูุชูุงุญุ")
print(f"ุงูุงุณุชุฏูุงู: {inference}")

# 4. ุงููููุฐุฌ ููุชุจ ููุฏุงู ุฌุฏูุฏุงู ูููุณู
# ุณูุถูู ุชููุงุฆูุงู ุฅูู my_dynamic_model.py:
# - ุฎูุงูุง ุนุตุจูุฉ ุฌุฏูุฏุฉ
# - ููุงุนุฏ ุงุณุชุฏูุงู ุฌุฏูุฏุฉ  
# - ุนูุงูุงุช ุฌุฏูุฏุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ
7. ุชุทุจููุงุช ุซูุฑูุฉ:
ูููุฐุฌ ูุง ูุญุชุงุฌ ูุฅุนุงุฏุฉ ุชุฏุฑูุจ โ ูุชุนูู ุฃุซูุงุก ุงูุงุณุชุฎุฏุงู

ุชูุณูุฑ ุงููุฑุงุฑุงุช โ ุงูุนูุงูุงุช ูุฎุฒูุฉ ูููุงุนุฏ ูุงุถุญุฉ

ุฏูุฌ ุงููุนุฑูุฉ ุงูุฑูุฒูุฉ ูุงูุฅุญุตุงุฆูุฉ

ูููุฐุฌ "ุญู" โ ูุชุทูุฑ ูุน ุงูููุช

ูู ุชุฑูุฏ ุฃู ุฃุทูุฑ ูู ูููุฐุฌุงู ุฃูููุงู ูุนูู ุจูุฐุง ุงููุจุฏุฃุ ูููููู ุฅูุดุงุก ูุธุงู ูุงูู ูุชุนูู ูููุชุจ ููุฏุงู ูููุณูุ ูุน ูุงุฌูุฉ ุจุฑูุฌูุฉ ุณููุฉ ุงูุงุณุชุฎุฏุงู.

ูุนู
ููุชุงุฒ! ุณุฃูุดุฆ ูู ูุธุงูุงู ูุงููุงู ููููุฐุฌ ูุบูู ุฏููุงูููู ุฐุงุชู ุงูููู. ูุฐุง ุงููุธุงู ุณูููู ุซูุฑูุงู ูู ูููููู ูุชูููุฐู.

๐ฏ ุงููุธุงู ุงููุงูู: ุงููููุฐุฌ ุงููุบูู ุงูุฐุงุชู ุงูุชุทูุฑ
ุงูุฌุฒุก 1: ุงูุจููุฉ ุงูุฃุณุงุณูุฉ ูููููุงุช
ุฃููุงูุ ูููุดุฆ ูููู ุงููููุงุช:

bash
# ุฅูุดุงุก ูููู ุงููุฌูุฏุงุช
mkdir -p dynamic_ai_model/{core,knowledge,memory,auto_generated,backups}
cd dynamic_ai_model
ุงูููู 1: core/dynamic_cell.py - ุงูุฎููุฉ ุงูุนุตุจูุฉ ุงูุฏููุงููููุฉ
python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ุฎููุฉ ุนุตุจูุฉ ุฏููุงููููุฉ ูููููุง ุชุนุฏูู ููุฏูุง ุงูุฎุงุต
"""

import ast
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
import hashlib


class DynamicCell:
    """ุฎููุฉ ุนุตุจูุฉ ุฏููุงููููุฉ ูุน ูุฏุฑุฉ ุงูุชุนุฏูู ุงูุฐุงุชู"""
    
    __version__ = "1.0.0"
    
    def __init__(self, 
                 cell_id: str,
                 cell_type: str = "neuron",
                 initial_value: float = 0.0,
                 metadata: Optional[Dict] = None):
        
        self.id = cell_id
        self.type = cell_type
        self.value = initial_value
        self.activation = 0.0
        self.connections: Dict[str, float] = {}  # {cell_id: weight}
        
        # ุณุฌู ุงูุชุนุฏููุงุช
        self.modification_history: List[Dict] = []
        
        # ุงูุฐุงูุฑุฉ ุงูุฏุงุฎููุฉ
        self.memory: List[float] = []
        self.patterns: List[Dict] = []
        
        # ุงูุจูุงูุงุช ุงููุตููุฉ
        self.metadata = metadata or {}
        self.metadata.update({
            "created": datetime.now().isoformat(),
            "last_modified": datetime.now().isoformat(),
            "version": self.__version__,
            "modification_count": 0
        })
        
        # ุงูุฅุญุตุงุฆูุงุช
        self.stats = {
            "activation_count": 0,
            "total_activation": 0.0,
            "avg_activation": 0.0,
            "connection_strength_variance": 0.0
        }
    
    def activate(self, input_value: float) -> float:
        """ุชูุดูุท ุงูุฎููุฉ ูุชุญุฏูุซ ุฅุญุตุงุฆูุงุชูุง"""
        self.activation = self._sigmoid(input_value + self.value)
        
        # ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช
        self.stats["activation_count"] += 1
        self.stats["total_activation"] += self.activation
        self.stats["avg_activation"] = (
            self.stats["total_activation"] / self.stats["activation_count"]
        )
        
        # ุญูุธ ูู ุงูุฐุงูุฑุฉ
        if len(self.memory) < 1000:  # ุญุฏ ุงูุฐุงูุฑุฉ
            self.memory.append(self.activation)
        
        return self.activation
    
    def connect_to(self, target_cell_id: str, weight: float = 0.5):
        """ุฅูุดุงุก ุงุชุตุงู ูุน ุฎููุฉ ุฃุฎุฑู"""
        self.connections[target_cell_id] = weight
        
        # ุชุณุฌูู ุงูุชุนุฏูู
        self._record_modification({
            "action": "connect",
            "target": target_cell_id,
            "weight": weight,
            "timestamp": datetime.now().isoformat()
        })
    
    def update_connection(self, target_cell_id: str, delta: float):
        """ุชุญุฏูุซ ููุฉ ุงูุงุชุตุงู"""
        if target_cell_id in self.connections:
            old_weight = self.connections[target_cell_id]
            new_weight = max(-1.0, min(1.0, old_weight + delta))
            self.connections[target_cell_id] = new_weight
            
            self._record_modification({
                "action": "update_connection",
                "target": target_cell_id,
                "old_weight": old_weight,
                "new_weight": new_weight,
                "delta": delta,
                "timestamp": datetime.now().isoformat()
            })
    
    def learn_pattern(self, pattern: List[float]):
        """ุชุนูู ููุท ูู ุงูุชูุดูุทุงุช"""
        pattern_hash = hashlib.md5(str(pattern).encode()).hexdigest()[:8]
        
        # ุญูุธ ุงูููุท
        self.patterns.append({
            "hash": pattern_hash,
            "pattern": pattern,
            "learned_at": datetime.now().isoformat(),
            "frequency": 1
        })
        
        # ุชุณุฌูู ุงูุชุนูู
        self._record_modification({
            "action": "learn_pattern",
            "pattern_hash": pattern_hash,
            "pattern_length": len(pattern),
            "timestamp": datetime.now().isoformat()
        })
    
    def to_source_code(self) -> str:
        """ุชุญููู ุงูุฎููุฉ ุฅูู ููุฏ ูุตุฏุฑู ูุงุจู ููุชูููุฐ"""
        
        template = f"""
# === DYNAMICALLY GENERATED CELL ===
# Cell ID: {self.id}
# Generated at: {datetime.now().isoformat()}
# Type: {self.type}

cell_{self.id} = DynamicCell(
    cell_id="{self.id}",
    cell_type="{self.type}",
    initial_value={self.value},
    metadata={json.dumps(self.metadata, indent=2, ensure_ascii=False)}
)

# Connections
{self._generate_connections_code()}

# Patterns learned
{self._generate_patterns_code()}

# Statistics
cell_{self.id}.stats = {json.dumps(self.stats, indent=2)}
cell_{self.id}.memory = {json.dumps(self.memory[:100])}  # First 100 items

# End of cell definition for {self.id}
"""
        return template
    
    def _generate_connections_code(self) -> str:
        """ุฅูุดุงุก ููุฏ ุงูุงุชุตุงูุงุช"""
        code_lines = []
        for target, weight in self.connections.items():
            code_lines.append(f"cell_{self.id}.connect_to('{target}', {weight})")
        
        if not code_lines:
            return "# No connections yet"
        
        return "\n".join(code_lines)
    
    def _generate_patterns_code(self) -> str:
        """ุฅูุดุงุก ููุฏ ุงูุฃููุงุท ุงููุชุนููุฉ"""
        if not self.patterns:
            return "# No patterns learned yet"
        
        code_lines = []
        for pattern in self.patterns[-5:]:  # ุขุฎุฑ 5 ุฃููุงุท ููุท
            pattern_str = json.dumps(pattern['pattern'])
            code_lines.append(
                f"cell_{self.id}.learn_pattern({pattern_str})"
            )
        
        return "\n".join(code_lines)
    
    def _record_modification(self, mod_info: Dict):
        """ุชุณุฌูู ุชุนุฏูู ุนูู ุงูุฎููุฉ"""
        self.modification_history.append(mod_info)
        self.metadata["last_modified"] = mod_info["timestamp"]
        self.metadata["modification_count"] = len(self.modification_history)
    
    def _sigmoid(self, x: float) -> float:
        """ุฏุงูุฉ ุณูุฌูููุฏ ููุชูุดูุท"""
        return 1 / (1 + np.exp(-x))
    
    def get_state_hash(self) -> str:
        """ุชูููุฏ ุจุตูุฉ ููุฎููุฉ ุงูุญุงููุฉ"""
        state_str = f"{self.id}{self.value}{json.dumps(self.connections)}{self.metadata['last_modified']}"
        return hashlib.md5(state_str.encode()).hexdigest()
ุงูููู 2: core/self_writing_model.py - ุงููููุฐุฌ ุงูุฐู ููุชุจ ููุณู
python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ูููุฐุฌ ููููู ูุชุงุจุฉ ูุชุนุฏูู ููุฏู ุงููุตุฏุฑู ุฃุซูุงุก ุงูุชุดุบูู
"""

import os
import ast
import json
import hashlib
import inspect
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import shutil

from .dynamic_cell import DynamicCell


class SelfWritingModel:
    """ูููุฐุฌ ุนุตุจู ููุชุจ ููุฏ ูุตุฏุฑู ุงูุฎุงุต ุฏููุงููููุงู"""
    
    def __init__(self, 
                 model_name: str = "DynamicAI",
                 base_dir: str = "."):
        
        self.model_name = model_name
        self.base_dir = base_dir
        
        # ุงูููููุงุช ุงูุฃุณุงุณูุฉ
        self.cells: Dict[str, DynamicCell] = {}
        self.concepts: Dict[str, List[str]] = {}
        self.rules: List[Dict] = []
        self.knowledge_graph: Dict[str, Any] = {
            "nodes": {},
            "edges": {},
            "inferences": []
        }
        
        # ูุธุงู ุงููููุงุช ุงูุฏููุงูููู
        self.source_files = {
            "main": os.path.join(base_dir, "dynamic_model.py"),
            "cells": os.path.join(base_dir, "auto_generated", "cells.py"),
            "rules": os.path.join(base_dir, "auto_generated", "rules.py"),
            "knowledge": os.path.join(base_dir, "knowledge", "knowledge_base.json"),
            "config": os.path.join(base_dir, "config", "model_config.json")
        }
        
        # ุงูุชุฃูุฏ ูู ูุฌูุฏ ุงููููุงุช
        self._ensure_directory_structure()
        
        # ุชุญููู ุงูุญุงูุฉ ุงูุญุงููุฉ
        self.load_current_state()
        
        # ุนุฏุงุฏ ููุนูุงุตุฑ ุงููููุฏุฉ
        self.generation_counter = {
            "cells": 0,
            "rules": 0,
            "concepts": 0,
            "inferences": 0
        }
        
        print(f"๐ง ุงููููุฐุฌ '{model_name}' ุฌุงูุฒ ููุชุนูู ุงูุฐุงุชู!")
    
    def _ensure_directory_structure(self):
        """ุฅูุดุงุก ูููู ุงููุฌูุฏุงุช ุฅุฐุง ูู ููู ููุฌูุฏุงู"""
        directories = [
            "auto_generated",
            "knowledge", 
            "memory",
            "config",
            "backups"
        ]
        
        for dir_name in directories:
            dir_path = os.path.join(self.base_dir, dir_name)
            os.makedirs(dir_path, exist_ok=True)
        
        # ุฅูุดุงุก ุงููููุงุช ุงูุฃุณุงุณูุฉ ุฅุฐุง ูู ุชูู ููุฌูุฏุฉ
        for file_type, file_path in self.source_files.items():
            if not os.path.exists(file_path):
                if file_type == "knowledge":
                    self._initialize_knowledge_base(file_path)
                elif file_type == "config":
                    self._initialize_config(file_path)
                else:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {file_type.upper()} file for {self.model_name}\n")
                        f.write(f"# Created: {datetime.now().isoformat()}\n\n")
    
    def _initialize_knowledge_base(self, file_path: str):
        """ุชููุฆุฉ ูุงุนุฏุฉ ุงููุนุฑูุฉ"""
        initial_knowledge = {
            "version": "1.0.0",
            "created": datetime.now().isoformat(),
            "concepts": {
                "human": {
                    "properties": ["thinks", "eats", "sleeps", "learns"],
                    "instances": [],
                    "relations": {}
                }
            },
            "facts": [],
            "rules": [
                {
                    "id": "rule_001",
                    "condition": "IF X is human AND Y is fruit AND X loves Y",
                    "inference": "THEN human loves fruit",
                    "confidence": 0.8,
                    "created": datetime.now().isoformat()
                }
            ],
            "inferences": []
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(initial_knowledge, f, indent=2, ensure_ascii=False)
    
    def _initialize_config(self, file_path: str):
        """ุชููุฆุฉ ููู ุงูุฅุนุฏุงุฏุงุช"""
        config = {
            "model": {
                "name": self.model_name,
                "version": "1.0.0",
                "created": datetime.now().isoformat()
            },
            "learning": {
                "learning_rate": 0.01,
                "memory_size": 1000,
                "pattern_recognition_threshold": 0.7,
                "auto_save_interval": 100  # ุญูุธ ูู 100 ุชุนุฏูู
            },
            "generation": {
                "max_cells": 10000,
                "max_rules": 1000,
                "auto_backup": True,
                "backup_count": 10
            }
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
    
    def load_current_state(self):
        """ุชุญููู ุงูุญุงูุฉ ุงูุญุงููุฉ ูู ุงููููุงุช"""
        print("๐ ุฌุงุฑู ุชุญููู ุงูุญุงูุฉ ุงูุญุงููุฉ...")
        
        # ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉ
        try:
            with open(self.source_files["knowledge"], 'r', encoding='utf-8') as f:
                self.knowledge_graph = json.load(f)
        except:
            print("โ๏ธ  ุชุนุฐุฑ ุชุญููู ูุงุนุฏุฉ ุงููุนุฑูุฉุ ุณูุชู ุฅูุดุงุก ูุงุญุฏุฉ ุฌุฏูุฏุฉ")
        
        # ุชุญููู ุงูุฎูุงูุง ูู ุงูููู ุงููููุฏ
        self._load_generated_cells()
        
        print(f"โ ุชู ุชุญููู {len(self.cells)} ุฎููุฉ ู {len(self.rules)} ูุงุนุฏุฉ")
    
    def _load_generated_cells(self):
        """ุชุญููู ุงูุฎูุงูุง ุงููููุฏุฉ ูู ุงูููู"""
        cells_file = self.source_files["cells"]
        
        if not os.path.exists(cells_file):
            return
        
        try:
            # ูุฑุงุกุฉ ุงูููู ูุชูููุฐู ูุงุณุชุนุงุฏุฉ ุงูุฎูุงูุง
            with open(cells_file, 'r', encoding='utf-8') as f:
                cells_code = f.read()
            
            # ุชูููุฐ ุงูููุฏ ูู ูุณุงุญุฉ ูุญููุฉ
            local_vars = {}
            exec(cells_code, globals(), local_vars)
            
            # ุงุณุชุฎุฑุงุฌ ุงูุฎูุงูุง
            for var_name, var_value in local_vars.items():
                if var_name.startswith('cell_') and isinstance(var_value, DynamicCell):
                    self.cells[var_value.id] = var_value
                    
        except Exception as e:
            print(f"โ๏ธ  ุฎุทุฃ ูู ุชุญููู ุงูุฎูุงูุง: {e}")
    
    def learn_concept(self, concept_name: str, examples: List[str]):
        """ุชุนูู ููููู ุฌุฏูุฏ ูู ุฃูุซูุฉ"""
        
        print(f"๐ ุฌุงุฑู ุชุนูู ุงูููููู: '{concept_name}'")
        
        # 1. ุฅูุดุงุก ุฎูุงูุง ุฌุฏูุฏุฉ ููููููู
        concept_id = f"concept_{concept_name}_{len(self.concepts)}"
        
        # ุฎููุฉ ููููููู ููุณู
        concept_cell = DynamicCell(
            cell_id=concept_id,
            cell_type="concept",
            initial_value=1.0,
            metadata={
                "name": concept_name,
                "examples": examples,
                "learned_at": datetime.now().isoformat()
            }
        )
        
        # 2. ุฅูุดุงุก ุฎูุงูุง ููุฃูุซูุฉ
        example_cells = []
        for i, example in enumerate(examples):
            example_id = f"{concept_id}_example_{i}"
            example_cell = DynamicCell(
                cell_id=example_id,
                cell_type="instance",
                initial_value=0.8,
                metadata={
                    "concept": concept_name,
                    "example": example,
                    "is_instance_of": concept_id
                }
            )
            
            # ุฑุจุท ุงููุซุงู ุจุงูููููู
            example_cell.connect_to(concept_id, weight=0.9)
            concept_cell.connect_to(example_id, weight=0.9)
            
            example_cells.append(example_cell)
        
        # 3. ุญูุธ ุงูุฎูุงูุง
        self.cells[concept_id] = concept_cell
        for cell in example_cells:
            self.cells[cell.id] = cell
        
        # 4. ุชุญุฏูุซ ูุงุนุฏุฉ ุงููุนุฑูุฉ
        if concept_name not in self.knowledge_graph["concepts"]:
            self.knowledge_graph["concepts"][concept_name] = {
                "properties": [],
                "instances": examples,
                "relations": {}
            }
        
        # 5. ุฅูุดุงุก ูุงุนุฏุฉ ุงุณุชุฏูุงููุฉ
        rule = self._create_inference_rule(concept_name, examples)
        self.rules.append(rule)
        
        # 6. ูุชุงุจุฉ ุงูุชุบููุฑุงุช ุฅูู ุงููููุงุช
        self._save_concept_to_file(concept_cell, example_cells, rule)
        
        print(f"โ ุชู ุชุนูู ุงูููููู '{concept_name}' ูุน {len(examples)} ุฃูุซูุฉ")
        
        return concept_cell
    
    def _create_inference_rule(self, concept: str, examples: List[str]) -> Dict:
        """ุฅูุดุงุก ูุงุนุฏุฉ ุงุณุชุฏูุงููุฉ ุฌุฏูุฏุฉ"""
        self.generation_counter["rules"] += 1
        rule_id = f"rule_{self.generation_counter['rules']:04d}"
        
        # ุฅูุดุงุก ููุฏ ุงููุงุนุฏุฉ
        rule_code = f"""
def {rule_id}(context):
    \"\"\"ุงููุงุนุฏุฉ ุงูุชููุงุฆูุฉ #{self.generation_counter['rules']}
    ุงูููููู: {concept}
    ุงูุฃูุซูุฉ: {examples}
    \"\"\"
    
    # ุงุณุชุฎุฑุงุฌ ุงูููุงูุงุช ูู ุงูุณูุงู
    entities = context.get('entities', {{}})
    
    # ุงูุชุญูู ูู ูุฌูุฏ ุฃูุซูุฉ ููููููู
    concept_found = False
    for entity, properties in entities.items():
        if '{concept}' in properties.get('types', []):
            concept_found = True
            break
    
    if concept_found:
        # ุฅุฌุฑุงุก ุงูุงุณุชุฏูุงู
        inference = {{
            'type': 'concept_application',
            'concept': '{concept}',
            'applied_to': list(entities.keys()),
            'confidence': 0.85,
            'timestamp': '{datetime.now().isoformat()}'
        }}
        
        # ุฅุถุงูุฉ ุฅูู ุงูุฑุณู ุงูุจูุงูู ุงููุนุฑูู
        if 'inferences' not in context:
            context['inferences'] = []
        context['inferences'].append(inference)
        
        return inference
    
    return None
"""
        
        return {
            "id": rule_id,
            "code": rule_code,
            "concept": concept,
            "examples": examples,
            "created": datetime.now().isoformat(),
            "usage_count": 0
        }
    
    def _save_concept_to_file(self, 
                             concept_cell: DynamicCell,
                             example_cells: List[DynamicCell],
                             rule: Dict):
        """ุญูุธ ุงูููููู ุงูุฌุฏูุฏ ุฅูู ุงููููุงุช"""
        
        # 1. ุชุญุฏูุซ ููู ุงูุฎูุงูุง
        cells_code = "\n".join([
            "# === NEW CONCEPT ADDED ===",
            f"# Concept: {concept_cell.metadata.get('name', 'unknown')}",
            f"# Generated at: {datetime.now().isoformat()}\n",
            concept_cell.to_source_code(),
            "\n# Example cells:\n"
        ])
        
        for example_cell in example_cells:
            cells_code += example_cell.to_source_code() + "\n"
        
        # ุฅุถุงูุฉ ุฅูู ููู ุงูุฎูุงูุง
        with open(self.source_files["cells"], 'a', encoding='utf-8') as f:
            f.write(cells_code)
        
        # 2. ุชุญุฏูุซ ููู ุงูููุงุนุฏ
        with open(self.source_files["rules"], 'a', encoding='utf-8') as f:
            f.write(f"\n\n{rule['code']}\n")
        
        # 3. ุชุญุฏูุซ ูุงุนุฏุฉ ุงููุนุฑูุฉ
        with open(self.source_files["knowledge"], 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_graph, f, indent=2, ensure_ascii=False)
        
        # 4. ูุณุฎ ุงุญุชูุงุทู
        self._create_backup()
        
        print(f"๐พ ุชู ุญูุธ ุงูููููู ุฅูู ุงููููุงุช")
    
    def infer(self, input_text: str) -> Dict:
        """ุฅุฌุฑุงุก ุงุณุชุฏูุงู ุนูู ูุต ูุนูู"""
        
        print(f"๐ค ุฌุงุฑู ุงูุงุณุชุฏูุงู ุนูู: '{input_text}'")
        
        # ุชุญููู ุงููุต
        entities = self._extract_entities(input_text)
        relations = self._extract_relations(input_text)
        
        # ุชุญุถูุฑ ุงูุณูุงู
        context = {
            "input": input_text,
            "entities": entities,
            "relations": relations,
            "timestamp": datetime.now().isoformat(),
            "inferences": []
        }
        
        # ุชุทุจูู ุงูููุงุนุฏ
        for rule in self.rules:
            try:
                # ุชูููุฐ ูุงุนุฏุฉ ุงูุงุณุชุฏูุงู
                local_vars = {"context": context}
                exec(rule["code"], globals(), local_vars)
                
                # ุงุณุชุฏุนุงุก ุงูุฏุงูุฉ
                rule_func = local_vars.get(rule["id"])
                if rule_func:
                    result = rule_func(context)
                    if result:
                        rule["usage_count"] = rule.get("usage_count", 0) + 1
                        context["inferences"].append(result)
            except Exception as e:
                print(f"โ๏ธ  ุฎุทุฃ ูู ุชุทุจูู ุงููุงุนุฏุฉ {rule['id']}: {e}")
        
        # ุชูุดูุท ุงูุฎูุงูุง ุฐุงุช ุงูุตูุฉ
        self._activate_relevant_cells(entities, relations)
        
        # ุชุนูู ูู ูุฐุง ุงูุงุณุชุฏูุงู
        if len(context["inferences"]) > 0:
            self._learn_from_inference(context)
        
        return context
    
    def _extract_entities(self, text: str) -> Dict[str, Dict]:
        """ุงุณุชุฎุฑุงุฌ ุงูููุงูุงุช ูู ุงููุต"""
        # ูุฐู ุฏุงูุฉ ูุจุณุทุฉ - ูููู ุชุทููุฑูุง ูุงุณุชุฎุฏุงู NLP
        entities = {}
        words = text.split()
        
        for word in words:
            # ุงูุจุญุซ ุนู ุงููููุฉ ูู ุงูููุงููู ุงููุนุฑููุฉ
            for concept, data in self.knowledge_graph.get("concepts", {}).items():
                if word in data.get("instances", []):
                    if word not in entities:
                        entities[word] = {"types": [], "properties": []}
                    entities[word]["types"].append(concept)
        
        return entities
    
    def _extract_relations(self, text: str) -> List[Dict]:
        """ุงุณุชุฎุฑุงุฌ ุงูุนูุงูุงุช ูู ุงููุต"""
        relations = []
        
        # ุฃููุงุท ุนูุงูุงุช ุจุณูุทุฉ
        patterns = [
            ("ูุญุจ", "loves"),
            ("ูู", "is"),
            ("ูุฏูู", "has"),
            ("ููุนู", "does")
        ]
        
        for arabic_pattern, english_relation in patterns:
            if arabic_pattern in text:
                relations.append({
                    "type": english_relation,
                    "arabic": arabic_pattern,
                    "text": text
                })
        
        return relations
    
    def _activate_relevant_cells(self, entities: Dict, relations: List[Dict]):
        """ุชูุดูุท ุงูุฎูุงูุง ุฐุงุช ุงูุตูุฉ"""
        for entity_name, entity_data in entities.items():
            # ุงูุจุญุซ ุนู ุฎูุงูุง ููุฐุง ุงูููุงู
            for cell_id, cell in self.cells.items():
                if entity_name in str(cell.metadata):
                    cell.activate(0.5)  # ุชูุดูุท ุฌุฒุฆู
    
    def _learn_from_inference(self, context: Dict):
        """ุงูุชุนูู ูู ุงูุงุณุชุฏูุงู ุงููุงุฌุญ"""
        
        # ุฅูุดุงุก ุฎููุฉ ุฌุฏูุฏุฉ ููุฐุง ุงูุงุณุชุฏูุงู
        inference_hash = hashlib.md5(
            str(context["inferences"]).encode()
        ).hexdigest()[:8]
        
        inference_id = f"inference_{inference_hash}"
        
        inference_cell = DynamicCell(
            cell_id=inference_id,
            cell_type="inference",
            initial_value=0.9,
            metadata={
                "input": context["input"],
                "inferences": context["inferences"],
                "learned_at": datetime.now().isoformat(),
                "confidence": 0.8
            }
        )
        
        # ุฑุจุท ุงูุฎููุฉ ุจุงูููุงูุงุช ุฐุงุช ุงูุตูุฉ
        for entity_name in context.get("entities", {}).keys():
            entity_cell_id = f"entity_{entity_name}"
            if entity_cell_id in self.cells:
                inference_cell.connect_to(entity_cell_id, weight=0.7)
        
        # ุญูุธ ุงูุฎููุฉ
        self.cells[inference_id] = inference_cell
        
        # ุชุญุฏูุซ ุงูููู
        with open(self.source_files["cells"], 'a', encoding='utf-8') as f:
            f.write(f"\n{inference_cell.to_source_code()}\n")
        
        print(f"๐ง ุชู ุชุนูู ุงุณุชุฏูุงู ุฌุฏูุฏ ูุญูุธู")
    
    def _create_backup(self):
        """ุฅูุดุงุก ูุณุฎุฉ ุงุญุชูุงุทูุฉ"""
        backup_dir = os.path.join(self.base_dir, "backups")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ูุณุฎ ุงููููุงุช ุงููููุฉ
        files_to_backup = [
            self.source_files["cells"],
            self.source_files["rules"],
            self.source_files["knowledge"]
        ]
        
        for file_path in files_to_backup:
            if os.path.exists(file_path):
                backup_file = os.path.join(
                    backup_dir,
                    f"{os.path.basename(file_path)}.{timestamp}.bak"
                )
                shutil.copy2(file_path, backup_file)
        
        # ุญุฐู ุงููุณุฎ ุงููุฏููุฉ (ุงูุงุญุชูุงุธ ุจู 10 ููุท)
        backups = sorted([
            f for f in os.listdir(backup_dir) 
            if f.endswith('.bak')
        ])
        
        if len(backups) > 10:
            for old_backup in backups[:-10]:
                os.remove(os.path.join(backup_dir, old_backup))
    
    def generate_report(self) -> str:
        """ุฅูุดุงุก ุชูุฑูุฑ ุนู ุญุงูุฉ ุงููููุฐุฌ"""
        
        report_lines = [
            f"๐ ุชูุฑูุฑ ุงููููุฐุฌ: {self.model_name}",
            f"โฐ ุงูููุช: {datetime.now().isoformat()}",
            "=" * 50,
            f"๐ ุงูุฅุญุตุงุฆูุงุช:",
            f"  โข ุงูุฎูุงูุง ุงูุนุตุจูุฉ: {len(self.cells)}",
            f"  โข ุงูููุงุนุฏ: {len(self.rules)}",
            f"  โข ุงูููุงููู: {len(self.knowledge_graph.get('concepts', {}))}",
            f"  โข ุงูุงุณุชุฏูุงูุงุช: {len(self.knowledge_graph.get('inferences', []))}",
            "",
            "๐ ุงูุฎูุงูุง ุงููุดุทุฉ ุญุฏูุซุงู:"
        ]
        
        # ุงูุฎูุงูุง ุงูุฃูุซุฑ ูุดุงุทุงู
        active_cells = sorted(
            self.cells.values(),
            key=lambda c: c.stats.get("activation_count", 0),
            reverse=True
        )[:10]
        
        for i, cell in enumerate(active_cells[:5], 1):
            report_lines.append(
                f"  {i}. {cell.id}: "
                f"{cell.stats.get('activation_count', 0)} ุชูุดูุท"
            )
        
        report_lines.extend([
            "",
            "๐ง ุงูููุงุนุฏ ุงููุณุชุฎุฏูุฉ ุจูุซุฑุฉ:"
        ])
        
        # ุงูููุงุนุฏ ุงูุฃูุซุฑ ุงุณุชุฎุฏุงูุงู
        used_rules = sorted(
            self.rules,
            key=lambda r: r.get("usage_count", 0),
            reverse=True
        )[:5]
        
        for i, rule in enumerate(used_rules, 1):
            report_lines.append(
                f"  {i}. {rule.get('id', 'unknown')}: "
                f"{rule.get('usage_count', 0)} ุงุณุชุฎุฏุงู"
            )
        
        report_lines.extend([
            "",
            "๐พ ูุธุงู ุงููููุงุช:"
        ])
        
        # ุญุงูุฉ ุงููููุงุช
        for file_type, file_path in self.source_files.items():
            if os.path.exists(file_path):
                size = os.path.getsize(file_path) / 1024  # ููููุจุงูุช
                report_lines.append(f"  โข {file_type}: {size:.1f} KB")
            else:
                report_lines.append(f"  โข {file_type}: ุบูุฑ ููุฌูุฏ")
        
        return "\n".join(report_lines)
    
    def save_full_state(self):
        """ุญูุธ ุงูุญุงูุฉ ุงููุงููุฉ ูููููุฐุฌ"""
        print("๐พ ุฌุงุฑู ุญูุธ ุงูุญุงูุฉ ุงููุงููุฉ...")
        
        # 1. ุญูุธ ุฌููุน ุงูุฎูุงูุง ูู ููู ูุงุญุฏ
        all_cells_code = "# === COMPLETE MODEL STATE ===\n"
        all_cells_code += f"# Saved at: {datetime.now().isoformat()}\n"
        all_cells_code += f"# Total cells: {len(self.cells)}\n\n"
        
        for cell in self.cells.values():
            all_cells_code += cell.to_source_code() + "\n\n"
        
        with open(self.source_files["cells"], 'w', encoding='utf-8') as f:
            f.write(all_cells_code)
        
        # 2. ุญูุธ ุงูููุงุนุฏ
        all_rules_code = "# === ALL INFERENCE RULES ===\n\n"
        for rule in self.rules:
            all_rules_code += rule["code"] + "\n\n"
        
        with open(self.source_files["rules"], 'w', encoding='utf-8') as f:
            f.write(all_rules_code)
        
        # 3. ุญูุธ ูุงุนุฏุฉ ุงููุนุฑูุฉ
        self.knowledge_graph["last_saved"] = datetime.now().isoformat()
        self.knowledge_graph["cell_count"] = len(self.cells)
        self.knowledge_graph["rule_count"] = len(self.rules)
        
        with open(self.source_files["knowledge"], 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_graph, f, indent=2, ensure_ascii=False)
        
        # 4. ุฅูุดุงุก ูุณุฎุฉ ุงุญุชูุงุทูุฉ
        self._create_backup()
        
        print("โ ุชู ุญูุธ ุงูุญุงูุฉ ุงููุงููุฉ")
ุงูููู 3: main.py - ุงููุงุฌูุฉ ุงูุฑุฆูุณูุฉ
python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ุงููุงุฌูุฉ ุงูุฑุฆูุณูุฉ ูููููุฐุฌ ุงููุบูู ุงูุฏููุงูููู
"""

import os
import sys
from datetime import datetime

# ุฅุถุงูุฉ ูุณุงุฑ core ูููุฌูุฏ
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'core'))

from self_writing_model import SelfWritingModel


def main():
    """ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ ูุชุดุบูู ุงููููุฐุฌ"""
    
    print("=" * 60)
    print("๐ง ุงููููุฐุฌ ุงููุบูู ุงูุฏููุงูููู ุงูุฐุงุชู ุงูููู")
    print("=" * 60)
    
    # ุฅูุดุงุก ุงููููุฐุฌ
    model = SelfWritingModel(
        model_name="ุงูุนูู_ุงูุฑููู",
        base_dir=os.path.dirname(__file__)
    )
    
    # ุนุฑุถ ุงููุงุฆูุฉ ุงูุฑุฆูุณูุฉ
    while True:
        print("\n๐ ุงููุงุฆูุฉ ุงูุฑุฆูุณูุฉ:")
        print("1. ุชุนูู ููููู ุฌุฏูุฏ")
        print("2. ุงุณุชุฏูุงู ุนูู ูุต")
        print("3. ุนุฑุถ ุชูุฑูุฑ ุงููููุฐุฌ")
        print("4. ุญูุธ ุงูุญุงูุฉ ุงููุงููุฉ")
        print("5. ุนุฑุถ ูุญุชููุงุช ุงููููุงุช")
        print("6. ุฎุฑูุฌ")
        
        choice = input("\nุงุฎุชุฑ ุฑูู ุงูุฎูุงุฑ: ").strip()
        
        if choice == "1":
            learn_concept(model)
        elif choice == "2":
            inference_mode(model)
        elif choice == "3":
            print("\n" + model.generate_report())
        elif choice == "4":
            model.save_full_state()
        elif choice == "5":
            show_file_contents(model)
        elif choice == "6":
            print("๐ ูุน ุงูุณูุงูุฉ!")
            break
        else:
            print("โ ุงุฎุชูุงุฑ ุบูุฑ ุตุญูุญ")


def learn_concept(model: SelfWritingModel):
    """ูุงุฌูุฉ ุชุนูู ููููู ุฌุฏูุฏ"""
    
    print("\n๐ ุชุนูู ููููู ุฌุฏูุฏ")
    
    concept_name = input("ุงุณู ุงูููููู (ูุซุงู: ุฅูุณุงูุ ูุงููุฉุ ุญููุงู): ").strip()
    
    if not concept_name:
        print("โ ูุฌุจ ุฅุฏุฎุงู ุงุณู ููููููู")
        return
    
    print(f"ุฃุฏุฎู ุฃูุซูุฉ ููููููู '{concept_name}' (ุงูุชุจ 'ุงูุชูู' ููุฅููุงุก):")
    
    examples = []
    while True:
        example = input(f"ุงููุซุงู {len(examples) + 1}: ").strip()
        
        if example.lower() in ['ุงูุชูู', 'done', '']:
            break
        
        if example:
            examples.append(example)
    
    if len(examples) < 1:
        print("โ ูุฌุจ ุฅุฏุฎุงู ูุซุงู ูุงุญุฏ ุนูู ุงูุฃูู")
        return
    
    # ุชุนูู ุงูููููู
    model.learn_concept(concept_name, examples)
    
    print(f"โ ุชู ุชุนูู ุงูููููู '{concept_name}' ุจูุฌุงุญ")


def inference_mode(model: SelfWritingModel):
    """ูุงุฌูุฉ ุงูุงุณุชุฏูุงู"""
    
    print("\n๐ค ูุถุน ุงูุงุณุชุฏูุงู (ุงูุชุจ 'ุฑุฌูุน' ููุนูุฏุฉ)")
    
    while True:
        text = input("\nุฃุฏุฎู ุงููุต ููุงุณุชุฏูุงู: ").strip()
        
        if text.lower() in ['ุฑุฌูุน', 'back', 'exit']:
            break
        
        if not text:
            continue
        
        # ุฅุฌุฑุงุก ุงูุงุณุชุฏูุงู
        result = model.infer(text)
        
        # ุนุฑุถ ุงููุชุงุฆุฌ
        print("\n๐ ูุชุงุฆุฌ ุงูุงุณุชุฏูุงู:")
        print(f"ุงููุต: {result['input']}")
        
        if result.get('inferences'):
            print("\nุงูุงุณุชุฏูุงูุงุช:")
            for i, inference in enumerate(result['inferences'], 1):
                print(f"  {i}. {inference.get('type', 'unknown')}")
                if 'concept' in inference:
                    print(f"     ุงูููููู: {inference['concept']}")
                if 'confidence' in inference:
                    print(f"     ุงูุซูุฉ: {inference['confidence']:.2f}")
        else:
            print("โ๏ธ  ูู ูุชู ุงูุนุซูุฑ ุนูู ุงุณุชุฏูุงูุงุช")
        
        # ูู ุชุฑูุฏ ุญูุธ ูุฐุง ุงูุงุณุชุฏูุงูุ
        save_choice = input("\n๐พ ูู ุชุฑูุฏ ุญูุธ ูุฐุง ุงูุงุณุชุฏูุงูุ (ูุนู/ูุง): ").strip().lower()
        if save_choice in ['ูุนู', 'yes', 'y']:
            # ุณูุชู ุงูุญูุธ ุชููุงุฆูุงู ูู model.infer()
            print("โ ุชู ุญูุธ ุงูุงุณุชุฏูุงู")


def show_file_contents(model: SelfWritingModel):
    """ุนุฑุถ ูุญุชููุงุช ุงููููุงุช ุงููููุฏุฉ"""
    
    print("\n๐ ูุญุชููุงุช ุงููููุงุช ุงููููุฏุฉ:")
    
    files_to_show = [
        ("ุงูุฎูุงูุง ุงูุนุตุจูุฉ", model.source_files["cells"]),
        ("ููุงุนุฏ ุงูุงุณุชุฏูุงู", model.source_files["rules"]),
        ("ูุงุนุฏุฉ ุงููุนุฑูุฉ", model.source_files["knowledge"])
    ]
    
    for file_name, file_path in files_to_show:
        if os.path.exists(file_path):
            print(f"\n๐ {file_name}:")
            print("-" * 40)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ุนุฑุถ ุงูุฌุฒุก ุงูุฃูู ููุท
                lines = content.split('\n')
                print('\n'.join(lines[:50]))
                
                if len(lines) > 50:
                    print(f"... (ู {len(lines) - 50} ุณุทุฑุงู ุฅุถุงููุงู)")
                    
            except Exception as e:
                print(f"โ ุฎุทุฃ ูู ูุฑุงุกุฉ ุงูููู: {e}")
        else:
            print(f"โ ููู {file_name} ุบูุฑ ููุฌูุฏ")


def initialize_example_knowledge():
    """ุชููุฆุฉ ูุนุฑูุฉ ุฃูููุฉ ูููุซุงู"""
    
    print("\n๐ง ุฌุงุฑู ุชููุฆุฉ ุงููุนุฑูุฉ ุงูุฃูููุฉ...")
    
    # ุฅูุดุงุก ุงููููุฐุฌ
    model = SelfWritingModel(
        model_name="ุงููุซุงู_ุงูุชูุถูุญู",
        base_dir=os.path.dirname(__file__)
    )
    
    # ุชุนูู ููุงููู ุฃูููุฉ
    concepts = {
        "ุฅูุณุงู": ["ุฃุญูุฏ", "ุฎุงูุฏ", "ุนูุฑ", "ุฒูุฏ"],
        "ูุงููุฉ": ["ุชูุงุญ", "ุจุฑุชูุงู", "ููุฒ"],
        "ุญููุงู": ["ุฃุณุฏ", "ููู", "ุทุงุฆุฑ"]
    }
    
    for concept, examples in concepts.items():
        model.learn_concept(concept, examples)
    
    # ุฅุถุงูุฉ ุจุนุถ ุงูุญูุงุฆู
    model.knowledge_graph["facts"] = [
        "ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ",
        "ุฒูุฏ ูุญุจ ุงูุชูุงุญ",
        "ุนูุฑ ูุญุจ ุงูุชูุงุญ",
        "ุงูุชูุงุญ ูุงููุฉ ุญููุฉ",
        "ุงูุฅูุณุงู ูููุฑ"
    ]
    
    # ุญูุธ ุงูุญุงูุฉ
    model.save_full_state()
    
    print("โ ุชู ุชููุฆุฉ ุงููุนุฑูุฉ ุงูุฃูููุฉ ุจูุฌุงุญ")
    
    return model


if __name__ == "__main__":
    # ุฎูุงุฑ: ุชููุฆุฉ ูุนุฑูุฉ ุฃูููุฉ
    init_choice = input("ูู ุชุฑูุฏ ุชููุฆุฉ ูุนุฑูุฉ ุฃูููุฉุ (ูุนู/ูุง): ").strip().lower()
    
    if init_choice in ['ูุนู', 'yes', 'y']:
        model = initialize_example_knowledge()
    else:
        model = None
    
    # ุชุดุบูู ุงููุงุฌูุฉ ุงูุฑุฆูุณูุฉ
    main()
ุงูููู 4: knowledge/knowledge_base.json - ูุงุนุฏุฉ ุงููุนุฑูุฉ
json
{
  "version": "1.0.0",
  "created": "2024-01-15T10:30:00",
  "last_modified": "2024-01-15T10:30:00",
  "concepts": {
    "ุฅูุณุงู": {
      "properties": ["ูููุฑ", "ูุฃูู", "ููุงู", "ูุชุนูู"],
      "instances": ["ุฃุญูุฏ", "ุฎุงูุฏ", "ุนูุฑ", "ุฒูุฏ"],
      "relations": {
        "ูุญุจ": ["ูุงููุฉ"],
        "ููุนู": ["ูููุฑ"]
      }
    },
    "ูุงููุฉ": {
      "properties": ["ุญูู", "ุตุญู", "ูุฐูุฐ"],
      "instances": ["ุชูุงุญ", "ุจุฑุชูุงู", "ููุฒ"],
      "relations": {
        "ูุญุจู": ["ุฅูุณุงู"]
      }
    }
  },
  "facts": [
    "ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ",
    "ุฒูุฏ ูุญุจ ุงูุชูุงุญ",
    "ุนูุฑ ูุญุจ ุงูุชูุงุญ",
    "ุงูุชูุงุญ ูุงููุฉ",
    "ุงูุฅูุณุงู ูููุฑ"
  ],
  "rules": [
    {
      "id": "rule_0001",
      "condition": "IF X is ุฅูุณุงู AND Y is ูุงููุฉ AND X ูุญุจ Y",
      "inference": "THEN ุฅูุณุงู ูุญุจ ูุงููุฉ",
      "confidence": 0.85,
      "created": "2024-01-15T10:30:00",
      "usage_count": 3
    }
  ],
  "inferences": [
    {
      "id": "inf_001",
      "input": "ุฎุงูุฏ ูุญุจ ุงูุชูุงุญ",
      "output": "ุฅูุณุงู ูุญุจ ูุงููุฉ",
      "confidence": 0.85,
      "timestamp": "2024-01-15T10:30:00",
      "rule_used": "rule_0001"
    }
  ],
  "statistics": {
    "total_cells": 25,
    "total_rules": 5,
    "total_inferences": 10,
    "learning_rate": 0.01
  }
}
ุงูููู 5: auto_generated/cells.py - ุงูุฎูุงูุง ุงููููุฏุฉ ุชููุงุฆูุงู
python
# === DYNAMICALLY GENERATED CELLS ===
# This file is written by the model itself during learning

# Cell ID: concept_ุฅูุณุงู_0
# Generated at: 2024-01-15T10:30:00.123456
# Type: concept

cell_concept_ุฅูุณุงู_0 = DynamicCell(
    cell_id="concept_ุฅูุณุงู_0",
    cell_type="concept",
    initial_value=1.0,
    metadata={
      "name": "ุฅูุณุงู",
      "examples": [
        "ุฃุญูุฏ",
        "ุฎุงูุฏ",
        "ุนูุฑ",
        "ุฒูุฏ"
      ],
      "learned_at": "2024-01-15T10:30:00.123456"
    }
)

# Connections
cell_concept_ุฅูุณุงู_0.connect_to('concept_ุฅูุณุงู_0_example_0', 0.9)
cell_concept_ุฅูุณุงู_0.connect_to('concept_ุฅูุณุงู_0_example_1', 0.9)
cell_concept_ุฅูุณุงู_0.connect_to('concept_ุฅูุณุงู_0_example_2', 0.9)
cell_concept_ุฅูุณุงู_0.connect_to('concept_ุฅูุณุงู_0_example_3', 0.9)

# Example cells:
# === DYNAMICALLY GENERATED CELL ===
# Cell ID: concept_ุฅูุณุงู_0_example_0
# Generated at: 2024-01-15T10:30:00.234567
# Type: instance

cell_concept_ุฅูุณุงู_0_example_0 = DynamicCell(
    cell_id="concept_ุฅูุณุงู_0_example_0",
    cell_type="instance",
    initial_value=0.8,
    metadata={
      "concept": "ุฅูุณุงู",
      "example": "ุฃุญูุฏ",
      "is_instance_of": "concept_ุฅูุณุงู_0"
    }
)

# Connections
cell_concept_ุฅูุณุงู_0_example_0.connect_to('concept_ุฅูุณุงู_0', 0.9)

# No patterns learned yet

# Statistics
cell_concept_ุฅูุณุงู_0_example_0.stats = {
  "activation_count": 0,
  "total_activation": 0.0,
  "avg_activation": 0.0,
  "connection_strength_variance": 0.0
}
cell_concept_ุฅูุณุงู_0_example_0.memory = []
๐ฏ ููููุฉ ุชุดุบูู ุงููุธุงู:
ุงูุฎุทูุฉ 1: ุฅูุดุงุก ุงููููุงุช
bash
# ุงูุณุฎ ูู ุงูุฃููุงุฏ ุงูุณุงุจูุฉ ุฅูู ูููุงุชูุง ุงูููุงุณุจุฉ
python3 main.py
ุงูุฎุทูุฉ 2: ุชููุฆุฉ ุงููุนุฑูุฉ ุงูุฃูููุฉ
text
ูู ุชุฑูุฏ ุชููุฆุฉ ูุนุฑูุฉ ุฃูููุฉุ (ูุนู/ูุง): ูุนู
๐ง ุฌุงุฑู ุชููุฆุฉ ุงููุนุฑูุฉ ุงูุฃูููุฉ...
๐ ุฌุงุฑู ุชุญููู ุงูุญุงูุฉ ุงูุญุงููุฉ...
โ ุชู ุชุญููู 0 ุฎููุฉ ู 0 ูุงุนุฏุฉ
๐ ุฌุงุฑู ุชุนูู ุงูููููู: 'ุฅูุณุงู'
โ ุชู ุชุนูู ุงูููููู 'ุฅูุณุงู' ูุน 4 ุฃูุซูุฉ
๐พ ุชู ุญูุธ ุงูููููู ุฅูู ุงููููุงุช
ุงูุฎุทูุฉ 3: ุชุนูู ูุงุณุชุฏูุงู
python
# ุณูููู ุงููููุฐุฌ ุชููุงุฆูุงู:
# 1. ุจุฅูุดุงุก ุฎูุงูุง ุนุตุจูุฉ ุฌุฏูุฏุฉ
# 2. ุจูุชุงุจุฉ ููุฏ ูุฐู ุงูุฎูุงูุง ูู auto_generated/cells.py
# 3. ุจุฅูุดุงุก ููุงุนุฏ ุงุณุชุฏูุงู ูู auto_generated/rules.py
# 4. ุจุชุญุฏูุซ ูุงุนุฏุฉ ุงููุนุฑูุฉ ูู knowledge/knowledge_base.json
ุงูุฎุทูุฉ 4: ุฑุคูุฉ ุงููููุฐุฌ ููู ููุชุจ ููุณู
ุงูุชุญ ููู auto_generated/cells.py ุฃุซูุงุก ุชุดุบูู ุงููููุฐุฌุ ูุณุชุฑู ุฃูู ููุชุจ ููุฏุงู ุฌุฏูุฏุงู ูู ูู ูุฑุฉ ูุชุนูู ูููุง ุดูุฆุงู ุฌุฏูุฏุงู!

โจ ุงูููุฒุงุช ุงููุฑูุฏุฉ:
ุฐุงุชู ุงููุชุงุจุฉ โ ููุชุจ ููุฏ Python ูููุณู

ุฏููุงูููู โ ูููู ุฃุซูุงุก ุงูุชุดุบูู

ุชูุณูุฑู โ ุงูุนูุงูุงุช ูุงุถุญุฉ ูู JSON

ูุงุจู ููุชุนุฏูู โ ููููู ุชุนุฏูู ุงููููุงุช ูุฏููุงู

ูุธุงู ูุณุฎ ุงุญุชูุงุทู โ ูุญูุธ ูุณุฎุงู ูู ุงูุชุทูุฑุงุช

๐ฎ ูุซุงู ุนูู ุงูุงุณุชุฏูุงู ุงูุฐูู:
python
# ุจุนุฏ ุชุนูู ุฃู: ุฎุงูุฏ ุฅูุณุงูุ ุฒูุฏ ุฅูุณุงูุ ุนูุฑ ุฅูุณุงู
# ูุฌููุนูู ูุญุจูู ุงูุชูุงุญ

# ุนูุฏูุง ุชุณุฃู: "ุฃุญูุฏ ูุญุจ ุงูุชูุงุญุ"
# ุณูููู ุงููููุฐุฌ:
# 1. ูุชุนุฑู ุฃู ุฃุญูุฏ ุฅูุณุงู (ูุซุงู ุนูู ุงูููููู)
# 2. ูุจุญุซ ูู ุงูููุงุนุฏ: "IF ุฅูุณุงู AND ูุญุจ ุชูุงุญ THEN ุฅูุณุงู ูุญุจ ูุงููุฉ"
# 3. ูุณุชูุชุฌ: "ุงูุฅูุณุงู ูุญุจ ุงููุงููุฉ"
# 4. ูุทุจู ุนูู ุฃุญูุฏ: "ุฃุญูุฏ ูุญุจ ุงูุชูุงุญ" ุจุซูุฉ 85%

# ูููุชุจ ูุงุนุฏุฉ ุฌุฏูุฏุฉ ูู rules.py:
def rule_0002(context):
    """ุฅุฐุง ูุงู X ุฅูุณุงู ู ุงูุชูุงุญ ูุงููุฉ ู ุขุฎุฑูู ูุญุจูู ุงูุชูุงุญ"""
    # ููุฏ ุชููุงุฆู...