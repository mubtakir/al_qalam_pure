# ğŸ“– Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Al-Qalam V4.1

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙŠØ´Ø±Ø­ ÙƒÙŠÙÙŠØ© ØªØ¯Ø±ÙŠØ¨ **Pure Dynamic Transformer** - Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ÙŠ ÙŠØ­ÙØ¸ Ø£ÙˆØ²Ø§Ù†Ù‡ ÙƒÙƒÙˆØ¯ Python ÙÙ‚Ø·.

---

## Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

```
training/
â”œâ”€â”€ linguistic/
â”‚   â”œâ”€â”€ linguistic_corpus.txt    # 910 MB (Ø¹Ø±Ø¨ÙŠ)
â”‚   â””â”€â”€ english_corpus.txt       # 232 MB
â”œâ”€â”€ data/
â”‚   â””â”€â”€ arabic_sample.txt        # 3 KB (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
â””â”€â”€ code/
```

---

## Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª

```
- Python 3.8+
- PyTorch 2.0+
- GPU (Ù…ÙˆØµÙ‰ Ø¨Ù‡): RTX 3060+ / 8GB VRAM
- RAM: 16GB+
```

---

## Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹

### 1. Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ (Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©)

```bash
python test_pure_code_training.py
```

### 2. ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„

```python
from core.dynamic_transformer.pure_dynamic_transformer import PureDynamicTransformer
from core.dynamic_transformer.pure_code_trainer import PureCodeTrainer
from core.dynamic_transformer.tokenizer import ArabicTokenizer

# 1. Tokenizer
tokenizer = ArabicTokenizer(vocab_size=32000)

def lines(path, max_lines=500000):
    with open(path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= max_lines: break
            yield line.strip()

tokenizer.build_vocab(
    lines("training/linguistic/linguistic_corpus.txt"),
    min_freq=3
)

# 2. Model
model = PureDynamicTransformer(
    vocab_size=len(tokenizer.word2idx),
    dim=256,
    num_heads=8,
    num_layers=6
)

# 3. Train
trainer = PureCodeTrainer(model, tokenizer, save_dir="my_model")
trainer.train(
    train_file="training/linguistic/linguistic_corpus.txt",
    epochs=3,
    batch_size=32,
    seq_len=128,
    learning_rate=3e-4
)
```

---

## Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª

| Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ | Ø§Ù„ÙˆØµÙ | Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© |
|---------|-------|-------------------|
| `dim` | Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ | 256 |
| `num_heads` | Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ | 8 |
| `num_layers` | Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª | 6 |
| `vocab_size` | Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª | 32000 |
| `batch_size` | Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© | 32 |
| `seq_len` | Ø·ÙˆÙ„ Ø§Ù„Ø³Ù„Ø³Ù„Ø© | 128 |
| `learning_rate` | Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… | 3e-4 |
| `epochs` | Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨ | 3 |

---

## Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹

| Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | GPU | Ø§Ù„ÙˆÙ‚Øª |
|----------|-----|-------|
| 3 KB (sample) | RTX 3060 | < 1 min |
| 100 MB | RTX 3060 | ~1 hour |
| 910 MB | RTX 3060 | ~6-12 hours |
| 910 MB | RTX 4090 | ~2-4 hours |

---

## Ø§Ù„Ù†ØªÙŠØ¬Ø©

```
my_model/final/
â”œâ”€â”€ config.py      # Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
â”‚   â†’ VOCAB_SIZE, DIM, NUM_LAYERS...
â”‚
â”œâ”€â”€ vocab.py       # Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
â”‚   â†’ WORD2IDX = {"Ø§Ù„Ù‚Ø·": 4, ...}
â”‚
â”œâ”€â”€ weights.py     # Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙƒÙƒÙˆØ¯!
â”‚   â†’ WEIGHTS = {"embedding.weight": ("eJz...", (32000, 256)), ...}
â”‚
â””â”€â”€ loader.py      # Ù„Ù„ØªØ­Ù…ÙŠÙ„
    â†’ load_model() â†’ (model, word2idx, idx2word)
```

**Ù„Ø§ `.pt` Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚!** âœ…

---

## Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

```python
# Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø§ØªØ¬
from my_model.final import load_model

model, word2idx, idx2word = load_model()

# Ø§Ù„ØªÙˆÙ„ÙŠØ¯
import torch
prompt = [word2idx.get("Ø§Ù„Ù‚Ø·", 1)]
generated = model.generate(torch.tensor([prompt]), max_new_tokens=20)
output = " ".join([idx2word.get(i, "?") for i in generated[0].tolist()])
print(output)
```

---

## Ø§Ù„ÙØ±Ù‚ Ø¹Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ

| Ø§Ù„Ø¬Ø§Ù†Ø¨ | Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ | Al-Qalam |
|--------|----------|----------|
| **Ø§Ù„ØªØ¯Ø±ÙŠØ¨** | Backprop | Backprop âœ… |
| **Ø§Ù„Ø­ÙØ¸** | `torch.save()` â†’ `.pt` | `to_python_code()` â†’ `.py` |
| **Ø§Ù„Ù…Ù„Ù** | Ø«Ù†Ø§Ø¦ÙŠ (ØºÙŠØ± Ù…Ù‚Ø±ÙˆØ¡) | ÙƒÙˆØ¯ Python (Ù…Ù‚Ø±ÙˆØ¡) |
| **Ø§Ù„Ø­Ø¬Ù…** | ~X MB | ~X MB (Ù…Ø¶ØºÙˆØ·) |

---

## Ù†ØµØ§Ø¦Ø­

1. **Ø§Ø¨Ø¯Ø£ ØµØºÙŠØ±Ø§Ù‹**: Ø§Ø®ØªØ¨Ø± Ø¹Ù„Ù‰ `arabic_sample.txt` Ø£ÙˆÙ„Ø§Ù‹
2. **Ø±Ø§Ù‚Ø¨ Loss**: ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ†Ø®ÙØ¶ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹
3. **Ø§Ø­ÙØ¸ Ø¨Ø§Ù†ØªØ¸Ø§Ù…**: `save_every=1000` ÙŠØ­ÙØ¸ ÙƒÙ„ 1000 Ø®Ø·ÙˆØ©
4. **Ø§Ø³ØªØ®Ø¯Ù… GPU**: Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ CPU Ø¨Ø·ÙŠØ¡ Ø¬Ø¯Ø§Ù‹

---

## Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

| Ø§Ù„Ù…Ø´ÙƒÙ„Ø© | Ø§Ù„Ø­Ù„ |
|---------|------|
| `CUDA out of memory` | Ù‚Ù„Ù„ `batch_size` Ø£Ùˆ `seq_len` |
| `Loss is NaN` | Ù‚Ù„Ù„ `learning_rate` |
| Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ù…ØªÙƒØ±Ø± | Ø²Ø¯ `temperature` (Ù…Ø«Ù„Ø§Ù‹ 1.0) |
