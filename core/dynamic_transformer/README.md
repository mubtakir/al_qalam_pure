# ğŸš€ Dynamic Transformer - Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

**Al-Qalam Dynamic Transformer V4.1** - Ø£ÙˆÙ„ Transformer ÙŠØ®Ø²Ù† Ø£ÙˆØ²Ø§Ù†Ù‡ ÙƒÙƒÙˆØ¯ Python Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ù„ÙØ§Øª Ø«Ù†Ø§Ø¦ÙŠØ©.

```
Ø§Ù„ÙÙ„Ø³ÙØ©: Ø§Ù„ÙƒÙˆØ¯ = Ø§Ù„Ø£ÙˆØ²Ø§Ù†
```

---

## Ø§Ù„ÙØ±Ù‚ Ø¹Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©

| Ø§Ù„Ø¬Ø§Ù†Ø¨ | GPT/Llama/Qwen | Al-Qalam |
|--------|----------------|----------|
| **Ø§Ù„Ø£ÙˆØ²Ø§Ù†** | `.pt/.safetensors` (Ø«Ù†Ø§Ø¦ÙŠ) | `.py` (ÙƒÙˆØ¯ Python) |
| **Ù…Ù‚Ø±ÙˆØ¡ØŸ** | âŒ Ù„Ø§ | âœ… Ù†Ø¹Ù… |
| **Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ø¯ÙŠÙ„ØŸ** | âŒ Ù„Ø§ | âœ… Ù†Ø¹Ù… |
| **Ø§Ù„ØªØ¯Ø±ÙŠØ¨** | Backprop | Backprop âœ… |
| **Ø§Ù„Ø­ÙØ¸** | `torch.save()` | `to_python_code()` |

---

## Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          PureDynamicTransformer                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Embedding     â†’ (vocab_size, dim)               â”‚
â”‚  PosEncoding   â†’ (max_seq_len, dim)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ TransformerBlock Ã— num_layers           â”‚    â”‚
â”‚  â”‚   - LayerNorm1                          â”‚    â”‚
â”‚  â”‚   - Multi-Head Attention (QKV)          â”‚    â”‚
â”‚  â”‚   - LayerNorm2                          â”‚    â”‚
â”‚  â”‚   - FFN (Linear â†’ GELU â†’ Linear)        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  LayerNorm (output)                              â”‚
â”‚  LM_Head â†’ (dim, vocab_size)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ø§Ù„Ù…Ù„ÙØ§Øª

```
core/dynamic_transformer/
â”œâ”€â”€ pure_dynamic_transformer.py  # Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
â”œâ”€â”€ pure_code_trainer.py         # Ø§Ù„Ù…ÙØ¯Ø±Ù‘Ø¨
â”œâ”€â”€ tokenizer.py                 # Tokenizer Ø¹Ø±Ø¨ÙŠ
â”œâ”€â”€ transformer_torch.py         # Ù†Ø³Ø®Ø© PyTorch Ø¹Ø§Ø¯ÙŠØ©
â”œâ”€â”€ trainer.py                   # Ù…ÙØ¯Ø±Ù‘Ø¨ Ø¹Ø§Ø¯ÙŠ
â””â”€â”€ README.md                    # Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù
```

---

## Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹

### 1. Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬

```python
from core.dynamic_transformer.pure_dynamic_transformer import PureDynamicTransformer

model = PureDynamicTransformer(
    vocab_size=32000,    # Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
    dim=256,             # Ø§Ù„Ø¨Ø¹Ø¯
    num_heads=8,         # Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    num_layers=6,        # Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
    max_seq_len=256      # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„
)

print(f"Parameters: {model.count_parameters():,}")
```

### 2. Ø§Ù„ØªØ¯Ø±ÙŠØ¨

```python
from core.dynamic_transformer.pure_code_trainer import PureCodeTrainer
from core.dynamic_transformer.tokenizer import ArabicTokenizer

# Tokenizer
tokenizer = ArabicTokenizer(vocab_size=32000)
tokenizer.build_vocab(line_iterator("data.txt"), min_freq=3)

# Training
trainer = PureCodeTrainer(model, tokenizer, save_dir="output")
trainer.train(
    train_file="data.txt",
    epochs=3,
    batch_size=32,
    seq_len=128,
    learning_rate=3e-4
)
```

### 3. Ø§Ù„Ø­ÙØ¸ ÙƒÙƒÙˆØ¯

```python
model.to_python_code("output_model", tokenizer.word2idx)
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø©:**
```
output_model/
â”œâ”€â”€ config.py      # Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
â”œâ”€â”€ vocab.py       # Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
â”œâ”€â”€ weights.py     # Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙƒÙƒÙˆØ¯!
â””â”€â”€ loader.py      # Ù„Ù„ØªØ­Ù…ÙŠÙ„
```

### 4. Ø§Ù„ØªØ­Ù…ÙŠÙ„

```python
from output_model import load_model

model, word2idx, idx2word = load_model()
```

### 5. Ø§Ù„ØªÙˆÙ„ÙŠØ¯

```python
import torch

prompt = "Ø§Ù„Ù‚Ø·"
ids = [word2idx.get(w, 1) for w in tokenizer.tokenize(prompt)]
generated = model.generate(torch.tensor([ids]), max_new_tokens=20)
output = " ".join([idx2word[i] for i in generated[0].tolist()])
print(output)
```

---

## Ù†Ù…ÙˆØ°Ø¬ ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„

```python
# train_full_pure.py
import sys
sys.path.insert(0, ".")

from core.dynamic_transformer.pure_dynamic_transformer import PureDynamicTransformer
from core.dynamic_transformer.tokenizer import ArabicTokenizer
from core.dynamic_transformer.pure_code_trainer import PureCodeTrainer

# Config
DATA = "training/linguistic/linguistic_corpus.txt"
SAVE_DIR = "trained_model"
VOCAB_SIZE = 32000
DIM = 256
HEADS = 8
LAYERS = 6

# Tokenizer
print("Building tokenizer...")
tokenizer = ArabicTokenizer(vocab_size=VOCAB_SIZE)

def lines(path, max_lines=500000):
    with open(path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= max_lines: break
            yield line.strip()

tokenizer.build_vocab(lines(DATA), min_freq=3)
tokenizer.save(f"{SAVE_DIR}/tokenizer.json")

# Model
print("Creating model...")
model = PureDynamicTransformer(
    vocab_size=len(tokenizer.word2idx),
    dim=DIM,
    num_heads=HEADS,
    num_layers=LAYERS
)
print(f"Parameters: {model.count_parameters():,}")

# Train
trainer = PureCodeTrainer(model, tokenizer, save_dir=SAVE_DIR)
trainer.train(
    train_file=DATA,
    epochs=3,
    batch_size=32,
    seq_len=128,
    learning_rate=3e-4,
    log_every=100,
    save_every=1000
)

print("Done! Model saved as Python code.")
```

---

## ÙƒÙŠÙ ØªÙØ­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙƒÙƒÙˆØ¯ØŸ

```python
# weights.py (Ù…Ø«Ø§Ù„ Ù…Ø¨Ø³Ø·)
import base64
import zlib
import numpy as np

def decompress_array(encoded, shape):
    compressed = base64.b64decode(encoded)
    data = zlib.decompress(compressed)
    return np.frombuffer(data, dtype=np.float32).reshape(shape)

WEIGHTS = {
    "embedding.weight": ("eJzrDPBz5+WS4mJg...", (32000, 256)),
    "blocks.0.qkv.weight": ("eJzrDPBz5+WS...", (768, 256)),
    # ... Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù…Ø¶ØºÙˆØ·Ø©
}

def load_weights():
    return {name: decompress_array(data, shape) 
            for name, (data, shape) in WEIGHTS.items()}
```

---

## Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§

| Ø§Ù„Ø­Ø¬Ù… | dim | heads | layers | params | GPU Memory |
|-------|-----|-------|--------|--------|------------|
| ØµØºÙŠØ± | 128 | 4 | 2 | ~400K | < 1 GB |
| Ù…ØªÙˆØ³Ø· | 256 | 8 | 6 | ~20M | 2-4 GB |
| ÙƒØ¨ÙŠØ± | 512 | 8 | 12 | ~80M | 8-16 GB |

---

## Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ø³Ø±ÙŠØ¹

```bash
# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹
python test_pure_code_training.py

# ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
python -c "
from core.dynamic_transformer.pure_code_trainer import main
main()
" --data training/linguistic/linguistic_corpus.txt --epochs 3
```
